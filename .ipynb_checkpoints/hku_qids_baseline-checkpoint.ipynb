{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cy1IGAK7pJpz"
   },
   "source": [
    "### This is a simple LGB baseline. You can work for feature engineering.\n",
    "### The seed is 42, which will bring good luck!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-02-19T11:39:13.476656Z",
     "iopub.status.busy": "2023-02-19T11:39:13.476206Z",
     "iopub.status.idle": "2023-02-19T11:39:13.486675Z",
     "shell.execute_reply": "2023-02-19T11:39:13.485328Z",
     "shell.execute_reply.started": "2023-02-19T11:39:13.476619Z"
    },
    "id": "Xs3TRi_HpJqA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import lightgbm as lgb\n",
    "import time\n",
    "import datetime\n",
    "from numba import jit\n",
    "#from lightgbm import LGBMRegressor\n",
    "from multiprocessing import Pool\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "n_fold = 10\n",
    "group_gap = 31\n",
    "seed = 42\n",
    "\n",
    "#Chris' paths:\n",
    "#TRAIN_MARKET_PATH = '/content/drive/MyDrive/hku_qis/hku-qids-2023-quantitative-investment-competition/first_round_train_market_data.csv'\n",
    "#TRAIN_FUNADMENTAL_PATH = '/content/drive/MyDrive/hku_qis/hku-qids-2023-quantitative-investment-competition/first_round_train_fundamental_data.csv'\n",
    "#TRAIN_RETURN_PATH = '/content/drive/MyDrive/hku_qis/hku-qids-2023-quantitative-investment-competition/first_round_train_return_data.csv'\n",
    "\n",
    "#TEST_MARKET_PATH = '/content/drive/MyDrive/hku_qis/hku-qids-2023-quantitative-investment-competition/first_round_test_market_data.csv'\n",
    "#TEST_FUNADMENTAL_PATH = '/content/drive/MyDrive/hku_qis/hku-qids-2023-quantitative-investment-competition/first_round_test_fundamental_data.csv'\n",
    "\n",
    "#Freya's paths:\n",
    "TRAIN_MARKET_PATH = '/Users/75717/Downloads/273_Washu/first_round_train_market_data.csv'\n",
    "TRAIN_FUNADMENTAL_PATH = '/Users/75717/Downloads/273_Washu/first_round_train_fundamental_data.csv'\n",
    "TRAIN_RETURN_PATH = '/Users/75717/Downloads/273_Washu/first_round_train_return_data.csv'\n",
    "\n",
    "TEST_MARKET_PATH = '/Users/75717/Downloads/273_Washu/first_round_test_market_data.csv'\n",
    "TEST_FUNADMENTAL_PATH = '/Users/75717/Downloads/273_Washu/first_round_test_fundamental_data.csv'\n",
    "\n",
    "#Cynthia's paths:\n",
    "#TRAIN_MARKET_PATH = '/content/drive/MyDrive/hku_qis/hku-qids-2023-quantitative-investment-competition/first_round_train_market_data.csv'\n",
    "#TRAIN_FUNADMENTAL_PATH = '/content/drive/MyDrive/hku_qis/hku-qids-2023-quantitative-investment-competition/first_round_train_fundamental_data.csv'\n",
    "#TRAIN_RETURN_PATH = '/content/drive/MyDrive/hku_qis/hku-qids-2023-quantitative-investment-competition/first_round_train_return_data.csv'\n",
    "\n",
    "#TEST_MARKET_PATH = '/content/drive/MyDrive/hku_qis/hku-qids-2023-quantitative-investment-competition/first_round_test_market_data.csv'\n",
    "#TEST_FUNADMENTAL_PATH = '/content/drive/MyDrive/hku_qis/hku-qids-2023-quantitative-investment-competition/first_round_test_fundamental_data.csv'\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 6)\n",
    "pd.set_option('display.max_columns', 350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T11:39:14.994016Z",
     "iopub.status.busy": "2023-02-19T11:39:14.993573Z",
     "iopub.status.idle": "2023-02-19T11:39:20.332244Z",
     "shell.execute_reply": "2023-02-19T11:39:20.331197Z",
     "shell.execute_reply.started": "2023-02-19T11:39:14.993976Z"
    },
    "id": "RdPuh_tdpJqB"
   },
   "outputs": [],
   "source": [
    "#read data\n",
    "df_train_market = pd.read_csv(TRAIN_MARKET_PATH)\n",
    "df_train_return = pd.read_csv(TRAIN_RETURN_PATH)\n",
    "df_train_fundamental = pd.read_csv(TRAIN_FUNADMENTAL_PATH)\n",
    "\n",
    "df_test_market = pd.read_csv(TEST_MARKET_PATH)\n",
    "df_test_fundamental = pd.read_csv(TEST_FUNADMENTAL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T11:39:20.334662Z",
     "iopub.status.busy": "2023-02-19T11:39:20.33424Z",
     "iopub.status.idle": "2023-02-19T11:40:01.127892Z",
     "shell.execute_reply": "2023-02-19T11:40:01.12668Z",
     "shell.execute_reply.started": "2023-02-19T11:39:20.334626Z"
    },
    "id": "Jv4-5wXJpJqB"
   },
   "outputs": [],
   "source": [
    "#merge train dataset and test dataset\n",
    "def split_time(x):\n",
    "    df1 = x['date_time'].str.split('d', expand=True)\n",
    "    df1.columns=['code','s']\n",
    "    code = df1['code']\n",
    "    df1 = df1['s'].str.split('p', expand=True)\n",
    "    df1.columns=['day','time_step']\n",
    "    df2 = x['date_time'].str.rsplit('p', expand=True)\n",
    "    df2.columns=['day_s','s']\n",
    "    df1['day_s'] = df2['day_s']\n",
    "    df1['code'] = code\n",
    "    x = pd.concat([x,df1],axis=1)\n",
    "    \n",
    "    return x\n",
    "\n",
    "df_train_market = split_time(df_train_market)\n",
    "df = pd.merge(df_train_fundamental,df_train_market, left_on='date_time',right_on='day_s')  \n",
    "df = pd.merge(df,df_train_return, left_on='day_s',right_on='date_time')  \n",
    "\n",
    "df_test_market = split_time(df_test_market)\n",
    "test = pd.merge(df_test_fundamental,df_test_market, left_on='date_time',right_on='day_s')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T11:40:01.129791Z",
     "iopub.status.busy": "2023-02-19T11:40:01.129418Z",
     "iopub.status.idle": "2023-02-19T11:40:05.177691Z",
     "shell.execute_reply": "2023-02-19T11:40:05.176402Z",
     "shell.execute_reply.started": "2023-02-19T11:40:01.129759Z"
    },
    "id": "_XWIEwTopJqC"
   },
   "outputs": [],
   "source": [
    "#drop duplicates\n",
    "df = df.drop_duplicates(subset='day_s', keep='last').reset_index(drop=True)\n",
    "test = test.drop_duplicates(subset='day_s', keep='last').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T11:40:05.180387Z",
     "iopub.status.busy": "2023-02-19T11:40:05.179995Z",
     "iopub.status.idle": "2023-02-19T11:40:05.193826Z",
     "shell.execute_reply": "2023-02-19T11:40:05.19235Z",
     "shell.execute_reply.started": "2023-02-19T11:40:05.180348Z"
    },
    "id": "QUpx57DzpJqC"
   },
   "outputs": [],
   "source": [
    "def correlation(a, train_data):\n",
    "    \n",
    "    b = train_data.get_label()\n",
    "    \n",
    "    a = np.ravel(a)\n",
    "    b = np.ravel(b)\n",
    "\n",
    "    len_data = len(a)\n",
    "    mean_a = np.sum(a) / len_data\n",
    "    mean_b = np.sum(b) / len_data\n",
    "    var_a = np.sum(np.square(a - mean_a)) / len_data\n",
    "    var_b = np.sum(np.square(b - mean_b)) / len_data\n",
    "\n",
    "    cov = np.sum((a * b))/len_data - mean_a*mean_b\n",
    "    corr = cov / np.sqrt(var_a * var_b)\n",
    "\n",
    "    return 'corr', corr, True\n",
    "\n",
    "# For CV score calculation\n",
    "def corr_score(pred, valid):\n",
    "    len_data = len(pred)\n",
    "    mean_pred = np.sum(pred) / len_data\n",
    "    mean_valid = np.sum(valid) / len_data\n",
    "    var_pred = np.sum(np.square(pred - mean_pred)) / len_data\n",
    "    var_valid = np.sum(np.square(valid - mean_valid)) / len_data\n",
    "\n",
    "    cov = np.sum((pred * valid))/len_data - mean_pred*mean_valid\n",
    "    corr = cov / np.sqrt(var_pred * var_valid)\n",
    "\n",
    "    return corr\n",
    "\n",
    "# For CV score calculation\n",
    "def wcorr_score(pred, valid, weight):\n",
    "    len_data = len(pred)\n",
    "    sum_w = np.sum(weight)\n",
    "    mean_pred = np.sum(pred * weight) / sum_w\n",
    "    mean_valid = np.sum(valid * weight) / sum_w\n",
    "    var_pred = np.sum(weight * np.square(pred - mean_pred)) / sum_w\n",
    "    var_valid = np.sum(weight * np.square(valid - mean_valid)) / sum_w\n",
    "\n",
    "    cov = np.sum((pred * valid * weight)) / sum_w - mean_pred*mean_valid\n",
    "    corr = cov / np.sqrt(var_pred * var_valid)\n",
    "\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "execution": {
     "iopub.execute_input": "2023-02-19T11:40:19.389021Z",
     "iopub.status.busy": "2023-02-19T11:40:19.387779Z",
     "iopub.status.idle": "2023-02-19T11:40:19.429486Z",
     "shell.execute_reply": "2023-02-19T11:40:19.428403Z",
     "shell.execute_reply.started": "2023-02-19T11:40:19.388979Z"
    },
    "id": "B-9usxYEpJqC",
    "outputId": "37c706cd-3617-438f-9ce0-33cd01b1248b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date_time_x', 'turnoverRatio', 'transactionAmount', 'pe_ttm', 'pe',\n",
       "       'pb', 'ps', 'pcf', 'date_time_y', 'open', 'close', 'high', 'low',\n",
       "       'volume', 'money', 'day', 'time_step', 'day_s', 'code', 'date_time',\n",
       "       'return'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date_time_x', 'turnoverRatio', 'transactionAmount', 'pe_ttm', 'pe',\n",
       "       'pb', 'ps', 'pcf', 'date_time_y', 'open', 'close', 'high', 'low',\n",
       "       'volume', 'money', 'day', 'time_step', 'day_s', 'code'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for normalizing data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#because these time variables are not number, they cannot be scaled\n",
    "col_train=[i for i in df.columns if i not in ['date_time_x', 'date_time_y', 'day', 'time_step', 'day_s', 'code', 'date_time','return']]\n",
    "timer_train=df.loc[:,['date_time_x', 'date_time_y', 'day', 'time_step', 'day_s', 'code', 'date_time','return']]\n",
    "#scale those x variables in training dataset\n",
    "scaled_df=scaler.fit_transform(df[col_train])\n",
    "scaled_df=pd.DataFrame(scaled_df,columns=['turnoverRatio', 'transactionAmount', 'pe_ttm', 'pe', 'pb', 'ps', 'pcf','open', 'close', 'high', 'low', 'volume', 'money'])\n",
    "#add timer back to the df\n",
    "new_df=pd.merge(scaled_df,timer_train,how='outer',left_index=True,right_index=True)\n",
    "\n",
    "#same process for test dataset\n",
    "col_test=[i for i in test.columns if i not in ['date_time_x', 'date_time_y', 'day', 'time_step', 'day_s', 'code', 'date_time']]\n",
    "timer_test=test.loc[:,['date_time_x', 'date_time_y', 'day', 'time_step', 'day_s', 'code']]\n",
    "scaled_test=scaler.fit_transform(test[col_test])\n",
    "scaled_test=pd.DataFrame(scaled_test,columns=['turnoverRatio', 'transactionAmount', 'pe_ttm', 'pe', 'pb', 'ps', 'pcf','open', 'close', 'high', 'low', 'volume', 'money'])\n",
    "new_test=pd.merge(scaled_test,timer_test,how='outer',left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>turnoverRatio</th>\n",
       "      <th>transactionAmount</th>\n",
       "      <th>pe_ttm</th>\n",
       "      <th>pe</th>\n",
       "      <th>pb</th>\n",
       "      <th>ps</th>\n",
       "      <th>pcf</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>volume</th>\n",
       "      <th>money</th>\n",
       "      <th>date_time_x</th>\n",
       "      <th>date_time_y</th>\n",
       "      <th>day</th>\n",
       "      <th>time_step</th>\n",
       "      <th>day_s</th>\n",
       "      <th>code</th>\n",
       "      <th>date_time</th>\n",
       "      <th>return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.093616</td>\n",
       "      <td>0.051792</td>\n",
       "      <td>0.025331</td>\n",
       "      <td>0.046411</td>\n",
       "      <td>0.163548</td>\n",
       "      <td>0.083384</td>\n",
       "      <td>0.948034</td>\n",
       "      <td>0.024360</td>\n",
       "      <td>0.024333</td>\n",
       "      <td>0.024332</td>\n",
       "      <td>0.024362</td>\n",
       "      <td>0.004363</td>\n",
       "      <td>0.009410</td>\n",
       "      <td>s0d1</td>\n",
       "      <td>s0d1p50</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>s0d1</td>\n",
       "      <td>s0</td>\n",
       "      <td>s0d1</td>\n",
       "      <td>-0.026877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.063802</td>\n",
       "      <td>0.011141</td>\n",
       "      <td>0.025208</td>\n",
       "      <td>0.046202</td>\n",
       "      <td>0.168140</td>\n",
       "      <td>0.065664</td>\n",
       "      <td>0.951514</td>\n",
       "      <td>0.015426</td>\n",
       "      <td>0.015374</td>\n",
       "      <td>0.015532</td>\n",
       "      <td>0.015375</td>\n",
       "      <td>0.001816</td>\n",
       "      <td>0.002596</td>\n",
       "      <td>s1d1</td>\n",
       "      <td>s1d1p50</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>s1d1</td>\n",
       "      <td>s1</td>\n",
       "      <td>s1d1</td>\n",
       "      <td>-0.052674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.032330</td>\n",
       "      <td>0.015439</td>\n",
       "      <td>0.025523</td>\n",
       "      <td>0.046811</td>\n",
       "      <td>0.158080</td>\n",
       "      <td>0.090780</td>\n",
       "      <td>0.951040</td>\n",
       "      <td>0.007680</td>\n",
       "      <td>0.007667</td>\n",
       "      <td>0.007667</td>\n",
       "      <td>0.007681</td>\n",
       "      <td>0.002155</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>s2d1</td>\n",
       "      <td>s2d1p50</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>s2d1</td>\n",
       "      <td>s2</td>\n",
       "      <td>s2d1</td>\n",
       "      <td>-0.002691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53889</th>\n",
       "      <td>0.028856</td>\n",
       "      <td>0.015569</td>\n",
       "      <td>0.024882</td>\n",
       "      <td>0.045675</td>\n",
       "      <td>0.019734</td>\n",
       "      <td>0.011614</td>\n",
       "      <td>0.951489</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.001897</td>\n",
       "      <td>0.001897</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.004846</td>\n",
       "      <td>0.001592</td>\n",
       "      <td>s51d998</td>\n",
       "      <td>s51d998p50</td>\n",
       "      <td>998</td>\n",
       "      <td>50</td>\n",
       "      <td>s51d998</td>\n",
       "      <td>s51</td>\n",
       "      <td>s51d998</td>\n",
       "      <td>-0.052286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53890</th>\n",
       "      <td>0.013962</td>\n",
       "      <td>0.022720</td>\n",
       "      <td>0.025208</td>\n",
       "      <td>0.046273</td>\n",
       "      <td>0.197897</td>\n",
       "      <td>0.081012</td>\n",
       "      <td>0.948013</td>\n",
       "      <td>0.029243</td>\n",
       "      <td>0.029194</td>\n",
       "      <td>0.029194</td>\n",
       "      <td>0.029245</td>\n",
       "      <td>0.001526</td>\n",
       "      <td>0.003895</td>\n",
       "      <td>s52d998</td>\n",
       "      <td>s52d998p50</td>\n",
       "      <td>998</td>\n",
       "      <td>50</td>\n",
       "      <td>s52d998</td>\n",
       "      <td>s52</td>\n",
       "      <td>s52d998</td>\n",
       "      <td>-0.015559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53891</th>\n",
       "      <td>0.032522</td>\n",
       "      <td>0.029102</td>\n",
       "      <td>0.024792</td>\n",
       "      <td>0.045511</td>\n",
       "      <td>0.043472</td>\n",
       "      <td>0.004854</td>\n",
       "      <td>0.952153</td>\n",
       "      <td>0.008670</td>\n",
       "      <td>0.008655</td>\n",
       "      <td>0.008655</td>\n",
       "      <td>0.008671</td>\n",
       "      <td>0.004737</td>\n",
       "      <td>0.004162</td>\n",
       "      <td>s53d998</td>\n",
       "      <td>s53d998p50</td>\n",
       "      <td>998</td>\n",
       "      <td>50</td>\n",
       "      <td>s53d998</td>\n",
       "      <td>s53</td>\n",
       "      <td>s53d998</td>\n",
       "      <td>-0.003662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53892 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       turnoverRatio  transactionAmount    pe_ttm        pe        pb  \\\n",
       "0           0.093616           0.051792  0.025331  0.046411  0.163548   \n",
       "1           0.063802           0.011141  0.025208  0.046202  0.168140   \n",
       "2           0.032330           0.015439  0.025523  0.046811  0.158080   \n",
       "...              ...                ...       ...       ...       ...   \n",
       "53889       0.028856           0.015569  0.024882  0.045675  0.019734   \n",
       "53890       0.013962           0.022720  0.025208  0.046273  0.197897   \n",
       "53891       0.032522           0.029102  0.024792  0.045511  0.043472   \n",
       "\n",
       "             ps       pcf      open     close      high       low    volume  \\\n",
       "0      0.083384  0.948034  0.024360  0.024333  0.024332  0.024362  0.004363   \n",
       "1      0.065664  0.951514  0.015426  0.015374  0.015532  0.015375  0.001816   \n",
       "2      0.090780  0.951040  0.007680  0.007667  0.007667  0.007681  0.002155   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "53889  0.011614  0.951489  0.001900  0.001897  0.001897  0.001900  0.004846   \n",
       "53890  0.081012  0.948013  0.029243  0.029194  0.029194  0.029245  0.001526   \n",
       "53891  0.004854  0.952153  0.008670  0.008655  0.008655  0.008671  0.004737   \n",
       "\n",
       "          money date_time_x date_time_y  day time_step    day_s code  \\\n",
       "0      0.009410        s0d1     s0d1p50    1        50     s0d1   s0   \n",
       "1      0.002596        s1d1     s1d1p50    1        50     s1d1   s1   \n",
       "2      0.001721        s2d1     s2d1p50    1        50     s2d1   s2   \n",
       "...         ...         ...         ...  ...       ...      ...  ...   \n",
       "53889  0.001592     s51d998  s51d998p50  998        50  s51d998  s51   \n",
       "53890  0.003895     s52d998  s52d998p50  998        50  s52d998  s52   \n",
       "53891  0.004162     s53d998  s53d998p50  998        50  s53d998  s53   \n",
       "\n",
       "      date_time    return  \n",
       "0          s0d1 -0.026877  \n",
       "1          s1d1 -0.052674  \n",
       "2          s2d1 -0.002691  \n",
       "...         ...       ...  \n",
       "53889   s51d998 -0.052286  \n",
       "53890   s52d998 -0.015559  \n",
       "53891   s53d998 -0.003662  \n",
       "\n",
       "[53892 rows x 21 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T11:42:06.156023Z",
     "iopub.status.busy": "2023-02-19T11:42:06.155626Z",
     "iopub.status.idle": "2023-02-19T11:42:06.169707Z",
     "shell.execute_reply": "2023-02-19T11:42:06.168505Z",
     "shell.execute_reply.started": "2023-02-19T11:42:06.155992Z"
    },
    "id": "jhkIqDnLpJqD"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(train,test):\n",
    "    # Hyperparammeters (just basic)\n",
    "    params = {\n",
    "      'objective': 'rmse',  \n",
    "      'boosting_type': 'gbdt',\n",
    "      'n_jobs': -1,\n",
    "      'verbose': -1\n",
    "    }\n",
    "    \n",
    "    # Split features and target\n",
    "    \n",
    "    x = train[[i for i in df.columns if i not in ['date_time_x', 'date_time_y', 'day', 'time_step', 'day_s', 'code', 'date_time','return']]]\n",
    "    y = train['return']\n",
    "    \n",
    "    x_test = test[[i for i in df.columns if i not in ['date_time_x', 'date_time_y', 'day', 'time_step', 'day_s', 'code', 'date_time','return']]]\n",
    "\n",
    "    oof_predictions = np.zeros(x.shape[0])\n",
    "    test_predictions = np.zeros(x_test.shape[0])\n",
    "    scores = []\n",
    "\n",
    "    # Create a KFold object\n",
    "    gkf = TimeSeriesSplit(n_splits=n_fold,gap=group_gap)\n",
    "    for fold, (trn_ind, val_ind) in enumerate(gkf.split(train['day'].values)):\n",
    "    \n",
    "        print(f'Training fold {fold + 1}')\n",
    "        x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n",
    "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
    "        \n",
    "        #这下面的用到lgb了\n",
    "        train_dataset = lgb.Dataset(x_train, y_train)\n",
    "        val_dataset = lgb.Dataset(x_val, y_val)\n",
    "        model = lgb.train(params = params, \n",
    "                          train_set = train_dataset, \n",
    "                          valid_sets = [train_dataset, val_dataset], \n",
    "                          num_boost_round = 200, \n",
    "                          early_stopping_rounds = 20, \n",
    "                          verbose_eval = False,\n",
    "                          feval = correlation)\n",
    "        # Add predictions to the out of folds array\n",
    "        \n",
    "        oof_predictions[val_ind] = model.predict(x_val)\n",
    "        \n",
    "        rmspe_score = corr_score(y_val,oof_predictions[val_ind])\n",
    "        print(f'Our out of folds corr_score is {rmspe_score}')\n",
    "        scores.append(rmspe_score)\n",
    "        test_predictions += model.predict(x_test) \n",
    "        \n",
    "    rmspe_score = corr_score(y, oof_predictions)\n",
    "    print(scores)\n",
    "    print(f'Our out of folds corr score is {rmspe_score}')\n",
    "    \n",
    "    # Return test predictions\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T11:42:06.156023Z",
     "iopub.status.busy": "2023-02-19T11:42:06.155626Z",
     "iopub.status.idle": "2023-02-19T11:42:06.169707Z",
     "shell.execute_reply": "2023-02-19T11:42:06.168505Z",
     "shell.execute_reply.started": "2023-02-19T11:42:06.155992Z"
    },
    "id": "jhkIqDnLpJqD"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(train,test):\n",
    "    # Hyperparammeters (just basic)\n",
    "    params = {\n",
    "      'objective': 'rmse',  \n",
    "      'boosting_type': 'gbdt',\n",
    "      'n_jobs': -1,\n",
    "      'verbose': -1\n",
    "    }\n",
    "    \n",
    "    # Split features and target\n",
    "    \n",
    "    x = train[[i for i in df.columns if i not in ['date_time_x', 'date_time_y', 'day', 'time_step', \n",
    "                                                  'day_s', 'code', 'date_time','return']]]\n",
    "    y = train['return']\n",
    "    \n",
    "    x_test = test[[i for i in df.columns if i not in ['date_time_x', 'date_time_y', 'day', 'time_step', \n",
    "                                                      'day_s', 'code', 'date_time','return']]]\n",
    "\n",
    "    oof_predictions = np.zeros(x.shape[0])\n",
    "    test_predictions = np.zeros(x_test.shape[0])\n",
    "    scores = []\n",
    "\n",
    "    # Create a KFold object\n",
    "    gkf = TimeSeriesSplit(n_splits=n_fold,gap=group_gap)\n",
    "    for fold, (trn_ind, val_ind) in enumerate(gkf.split(train['day'].values)):\n",
    "    \n",
    "        print(f'Training fold {fold + 1}')\n",
    "        x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n",
    "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
    "        \n",
    "        # create and fit the LSTM network\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1],1)))\n",
    "        model.add(LSTM(units=50))\n",
    "        model.add(Dense(1))\n",
    " \n",
    "        model.compile(loss='mean_squared_error', optimizer='adam', metrics='mae')\n",
    "        model.fit(x_train, y_train, epochs=1, batch_size=1, verbose=2,validation_data=(x_val,y_val))\n",
    "        \n",
    "        #This train_pred must have the same shape as the dataset on which you fitted the scaler. \n",
    "        #To do the inverse_transform you can extract the needed attributes from your scaler \n",
    "        #and apply them to your prediction.\n",
    "        scaler_pred=MinMaxScaler()\n",
    "        scaler_pred.min_, scaler_pred.scale_ = scaler.min_[1], scaler.scale_[1]\n",
    "        test_predictions += scaler_pred.inverse_transform(model.predict(x_test)) \n",
    "        \n",
    "    rmspe_score = corr_score(y, oof_predictions)\n",
    "    print(scores)\n",
    "    print(f'Our out of folds corr score is {rmspe_score}')\n",
    "    \n",
    "    # Return test predictions\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-02-19T11:42:06.401036Z",
     "iopub.status.busy": "2023-02-19T11:42:06.400583Z",
     "iopub.status.idle": "2023-02-19T11:42:08.132067Z",
     "shell.execute_reply": "2023-02-19T11:42:08.131046Z",
     "shell.execute_reply.started": "2023-02-19T11:42:06.401001Z"
    },
    "id": "s2MYJdoEpJqD",
    "outputId": "bce4f8ed-9413-4f63-8fff-6d27f951369e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "4871/4871 - 94s - loss: 0.0017 - mae: 0.0307 - val_loss: 0.0076 - val_mae: 0.0667 - 94s/epoch - 19ms/step\n",
      "Training fold 2\n",
      "9770/9770 - 168s - loss: 0.0046 - mae: 0.0488 - val_loss: 0.0022 - val_mae: 0.0351 - 168s/epoch - 17ms/step\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 10.6 GiB for an array with shape (37800, 37800) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-c64ad65ac5d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnew_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-671fa538e9be>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(train, test)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mscaler_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mscaler_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaler_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mtest_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_predictions\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mscaler_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mrmspe_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorr_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moof_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 10.6 GiB for an array with shape (37800, 37800) and data type float64"
     ]
    }
   ],
   "source": [
    "test_predictions = train_and_evaluate(new_df,new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T11:42:08.145003Z",
     "iopub.status.busy": "2023-02-19T11:42:08.144544Z",
     "iopub.status.idle": "2023-02-19T11:42:08.25156Z",
     "shell.execute_reply": "2023-02-19T11:42:08.250527Z",
     "shell.execute_reply.started": "2023-02-19T11:42:08.144966Z"
    },
    "id": "TGGyX9skpJqD"
   },
   "outputs": [],
   "source": [
    "# Save test predictions\n",
    "test['return'] = test_predictions\n",
    "\n",
    "prediction = test[['date_time_x','return']]\n",
    "prediction.columns=['date_time','return']\n",
    "prediction.to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NA8tjThw0EyC"
   },
   "outputs": [],
   "source": [
    "import random \n",
    "random.seed(20230206)\n",
    "\n",
    "SUBMISSION_PATH = '/kaggle/working/submission.csv'\n",
    "\n",
    "POINT_PER_DAY = 50\n",
    "\n",
    "class QIDS:\n",
    "    def __init__(self) -> None:\n",
    "        self.__submission_path = SUBMISSION_PATH\n",
    "        self.__current_idx = 0\n",
    "        self.__predict_idx = 0\n",
    "        self.__num_of_stocks = 54\n",
    "        self.__point_per_day = POINT_PER_DAY\n",
    "        self.__end = False\n",
    "        self.__current_fundamental_df = None\n",
    "\n",
    "        self.__fundamental_df = pd.read_csv(TEST_FUNADMENTAL_PATH)\n",
    "        self.__market_df = pd.read_csv(TEST_MARKET_PATH)\n",
    "        \n",
    "        if len(self.__fundamental_df) / self.__num_of_stocks != len(self.__market_df)/ self.__num_of_stocks / self.__point_per_day:\n",
    "            raise ValueError('The length of fundamental data and market data is not equal.')\n",
    "        self.__length = len(self.__fundamental_df) / self.__num_of_stocks\n",
    "\n",
    "        with open(self.__submission_path, 'w') as f:\n",
    "            f.write('date_time,return\\n') \n",
    "        \n",
    "        print('Environment is initialized.')\n",
    "    \n",
    "    def is_end(self):\n",
    "        return self.__end\n",
    "\n",
    "    # return the fun\n",
    "    def get_current_market(self):\n",
    "        if self.__end:\n",
    "            raise ValueError('The environment has ended.')\n",
    "\n",
    "        # check if the current index is equal to the predict index\n",
    "        if self.__current_idx != self.__predict_idx:\n",
    "            raise ValueError('The current index is not equal to the predict index.')\n",
    "\n",
    "        # load data of the current day\n",
    "        fundamental_df = self.__fundamental_df.iloc[self.__current_idx * self.__num_of_stocks: (self.__current_idx + 1) * self.__num_of_stocks]\n",
    "        market_df = self.__market_df.iloc[self.__current_idx * self.__num_of_stocks * self.__point_per_day: (self.__current_idx + 1) * self.__num_of_stocks * self.__point_per_day]\n",
    "        \n",
    "        # update the current index\n",
    "        self.__current_idx += 1\n",
    "        self.__current_fundamental_df = fundamental_df.reset_index()\n",
    "        \n",
    "        return fundamental_df, market_df\n",
    "\n",
    "    def input_prediction(self, predict_ds: pd.Series):\n",
    "        if self.__end:\n",
    "            raise ValueError('The environment has ended.')\n",
    "\n",
    "        # check if the current index is equal to the predict index plus 1\n",
    "        if self.__current_idx != self.__predict_idx + 1:\n",
    "            raise ValueError('The current index is not equal to the predict index plus 1.')\n",
    "\n",
    "        # check the length of the predict_ds\n",
    "        if len(predict_ds) != self.__num_of_stocks:\n",
    "            raise ValueError('The length of input decisions is wrong.')\n",
    "        \n",
    "        # check the type of the predict_ds\n",
    "        if type(predict_ds) != pd.Series:\n",
    "            raise TypeError('The type of input decisions is wrong.')\n",
    "        \n",
    "        # write the prediction to the submission file\n",
    "        with open(self.__submission_path, 'a') as f:\n",
    "            for idx in range(len(predict_ds)):\n",
    "                f.write(f\"{str(self.__current_fundamental_df['date_time'][idx])},{str(predict_ds.iloc[idx])}\\n\")\n",
    "\n",
    "                # must follow the stock order\n",
    "                # f.write(f\"s{idx}d{self.__current_idx},{str(predict_ds.iloc[idx])}\\n\")\n",
    "        \n",
    "        self.__predict_idx += 1\n",
    "        if self.__predict_idx == self.__length:\n",
    "            self.__end = True\n",
    "            print('Data Feeding is finished.')\n",
    "        \n",
    "\n",
    "# initialize the environment\n",
    "def make_env():\n",
    "    if random.random() == 0.8396457911824297:\n",
    "        return QIDS()\n",
    "    else:\n",
    "        raise ImportError('You cannot make this environment twice.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "id": "-7TOxUH4pJqD",
    "outputId": "ee0fd5dd-62f2-4b4a-f3ee-db9deadf51de"
   },
   "outputs": [],
   "source": [
    "#from qids_package.qids import *\n",
    "\n",
    "env = make_env()\n",
    "\n",
    "import random \n",
    "random.seed(42)\n",
    "\n",
    "while not env.is_end():\n",
    "\tfundamental_df, market_df = env.get_current_market()\n",
    "\t\n",
    "\tl = []\n",
    "\tfor idx in range(54):\n",
    "\t\tl.append(random.random())\n",
    "\tpredict_ds =pd.Series(1)\n",
    "\t\n",
    "\tenv.input_prediction(predict_ds)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
