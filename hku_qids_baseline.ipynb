{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cy1IGAK7pJpz"
   },
   "source": [
    "### This is a simple LGB baseline. You can work for feature engineering.\n",
    "### The seed is 42, which will bring good luck!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-02-19T11:39:13.476656Z",
     "iopub.status.busy": "2023-02-19T11:39:13.476206Z",
     "iopub.status.idle": "2023-02-19T11:39:13.486675Z",
     "shell.execute_reply": "2023-02-19T11:39:13.485328Z",
     "shell.execute_reply.started": "2023-02-19T11:39:13.476619Z"
    },
    "id": "Xs3TRi_HpJqA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import lightgbm as lgb\n",
    "import time\n",
    "import datetime\n",
    "from numba import jit\n",
    "#from lightgbm import LGBMRegressor\n",
    "from multiprocessing import Pool\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "n_fold = 10\n",
    "group_gap = 31\n",
    "seed = 42\n",
    "\n",
    "#Chris' paths:\n",
    "TRAIN_MARKET_PATH = '../HKU_QIS/first_round_train_market_data.csv'\n",
    "TRAIN_FUNADMENTAL_PATH = '../HKU_QIS/first_round_train_fundamental_data.csv'\n",
    "TRAIN_RETURN_PATH = '../HKU_QIS/first_round_train_return_data.csv'\n",
    "\n",
    "TEST_MARKET_PATH = '../HKU_QIS/qids_package/first_round_test_market_data.csv'\n",
    "TEST_FUNADMENTAL_PATH = '../HKU_QIS/qids_package/first_round_test_fundamental_data.csv'\n",
    "\n",
    "#Freya's paths:\n",
    "#TRAIN_MARKET_PATH = '/Users/75717/Downloads/273_Washu/first_round_train_market_data.csv'\n",
    "#TRAIN_FUNADMENTAL_PATH = '/Users/75717/Downloads/273_Washu/first_round_train_fundamental_data.csv'\n",
    "#TRAIN_RETURN_PATH = '/Users/75717/Downloads/273_Washu/first_round_train_return_data.csv'\n",
    "\n",
    "#TEST_MARKET_PATH = '/Users/75717/Downloads/273_Washu/first_round_test_market_data.csv'\n",
    "#TEST_FUNADMENTAL_PATH = '/Users/75717/Downloads/273_Washu/first_round_test_fundamental_data.csv'\n",
    "\n",
    "#Cynthia's paths:\n",
    "#TRAIN_MARKET_PATH = '/content/drive/MyDrive/hku_qis/hku-qids-2023-quantitative-investment-competition/first_round_train_market_data.csv'\n",
    "#TRAIN_FUNADMENTAL_PATH = '/content/drive/MyDrive/hku_qis/hku-qids-2023-quantitative-investment-competition/first_round_train_fundamental_data.csv'\n",
    "#TRAIN_RETURN_PATH = '/content/drive/MyDrive/hku_qis/hku-qids-2023-quantitative-investment-competition/first_round_train_return_data.csv'\n",
    "\n",
    "#TEST_MARKET_PATH = '/content/drive/MyDrive/hku_qis/hku-qids-2023-quantitative-investment-competition/first_round_test_market_data.csv'\n",
    "#TEST_FUNADMENTAL_PATH = '/content/drive/MyDrive/hku_qis/hku-qids-2023-quantitative-investment-competition/first_round_test_fundamental_data.csv'\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 6)\n",
    "pd.set_option('display.max_columns', 350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T11:39:14.994016Z",
     "iopub.status.busy": "2023-02-19T11:39:14.993573Z",
     "iopub.status.idle": "2023-02-19T11:39:20.332244Z",
     "shell.execute_reply": "2023-02-19T11:39:20.331197Z",
     "shell.execute_reply.started": "2023-02-19T11:39:14.993976Z"
    },
    "id": "RdPuh_tdpJqB"
   },
   "outputs": [],
   "source": [
    "#read data\n",
    "df_train_market = pd.read_csv(TRAIN_MARKET_PATH)\n",
    "df_train_return = pd.read_csv(TRAIN_RETURN_PATH)\n",
    "df_train_fundamental = pd.read_csv(TRAIN_FUNADMENTAL_PATH)\n",
    "\n",
    "df_test_market = pd.read_csv(TEST_MARKET_PATH)\n",
    "df_test_fundamental = pd.read_csv(TEST_FUNADMENTAL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T11:39:20.334662Z",
     "iopub.status.busy": "2023-02-19T11:39:20.33424Z",
     "iopub.status.idle": "2023-02-19T11:40:01.127892Z",
     "shell.execute_reply": "2023-02-19T11:40:01.12668Z",
     "shell.execute_reply.started": "2023-02-19T11:39:20.334626Z"
    },
    "id": "Jv4-5wXJpJqB"
   },
   "outputs": [],
   "source": [
    "#merge train dataset and test dataset\n",
    "def split_time(x):\n",
    "    df1 = x['date_time'].str.split('d', expand=True)\n",
    "    df1.columns=['code','s']\n",
    "    code = df1['code']\n",
    "    df1 = df1['s'].str.split('p', expand=True)\n",
    "    df1.columns=['day','time_step']\n",
    "    df2 = x['date_time'].str.rsplit('p', expand=True)\n",
    "    df2.columns=['day_s','s']\n",
    "    df1['day_s'] = df2['day_s']\n",
    "    df1['code'] = code\n",
    "    x = pd.concat([x,df1],axis=1)\n",
    "    \n",
    "    return x\n",
    "\n",
    "df_train_market = split_time(df_train_market)\n",
    "df = pd.merge(df_train_fundamental,df_train_market, left_on='date_time',right_on='day_s')  \n",
    "df = pd.merge(df,df_train_return, left_on='day_s',right_on='date_time')  \n",
    "\n",
    "df_test_market = split_time(df_test_market)\n",
    "test = pd.merge(df_test_fundamental,df_test_market, left_on='date_time',right_on='day_s')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T11:40:01.129791Z",
     "iopub.status.busy": "2023-02-19T11:40:01.129418Z",
     "iopub.status.idle": "2023-02-19T11:40:05.177691Z",
     "shell.execute_reply": "2023-02-19T11:40:05.176402Z",
     "shell.execute_reply.started": "2023-02-19T11:40:01.129759Z"
    },
    "id": "_XWIEwTopJqC"
   },
   "outputs": [],
   "source": [
    "#drop duplicates\n",
    "df = df.drop_duplicates(subset='day_s', keep='last').reset_index(drop=True)\n",
    "test = test.drop_duplicates(subset='day_s', keep='last').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T11:40:05.180387Z",
     "iopub.status.busy": "2023-02-19T11:40:05.179995Z",
     "iopub.status.idle": "2023-02-19T11:40:05.193826Z",
     "shell.execute_reply": "2023-02-19T11:40:05.19235Z",
     "shell.execute_reply.started": "2023-02-19T11:40:05.180348Z"
    },
    "id": "QUpx57DzpJqC"
   },
   "outputs": [],
   "source": [
    "def correlation(a, train_data):\n",
    "    \n",
    "    b = train_data.get_label()\n",
    "    \n",
    "    a = np.ravel(a)\n",
    "    b = np.ravel(b)\n",
    "\n",
    "    len_data = len(a)\n",
    "    mean_a = np.sum(a) / len_data\n",
    "    mean_b = np.sum(b) / len_data\n",
    "    var_a = np.sum(np.square(a - mean_a)) / len_data\n",
    "    var_b = np.sum(np.square(b - mean_b)) / len_data\n",
    "\n",
    "    cov = np.sum((a * b))/len_data - mean_a*mean_b\n",
    "    corr = cov / np.sqrt(var_a * var_b)\n",
    "\n",
    "    return 'corr', corr, True\n",
    "\n",
    "# For CV score calculation\n",
    "def corr_score(pred, valid):\n",
    "    len_data = len(pred)\n",
    "    mean_pred = np.sum(pred) / len_data\n",
    "    mean_valid = np.sum(valid) / len_data\n",
    "    var_pred = np.sum(np.square(pred - mean_pred)) / len_data\n",
    "    var_valid = np.sum(np.square(valid - mean_valid)) / len_data\n",
    "\n",
    "    cov = np.sum((pred * valid))/len_data - mean_pred*mean_valid\n",
    "    corr = cov / np.sqrt(var_pred * var_valid)\n",
    "\n",
    "    return corr\n",
    "\n",
    "# For CV score calculation\n",
    "def wcorr_score(pred, valid, weight):\n",
    "    len_data = len(pred)\n",
    "    sum_w = np.sum(weight)\n",
    "    mean_pred = np.sum(pred * weight) / sum_w\n",
    "    mean_valid = np.sum(valid * weight) / sum_w\n",
    "    var_pred = np.sum(weight * np.square(pred - mean_pred)) / sum_w\n",
    "    var_valid = np.sum(weight * np.square(valid - mean_valid)) / sum_w\n",
    "\n",
    "    cov = np.sum((pred * valid * weight)) / sum_w - mean_pred*mean_valid\n",
    "    corr = cov / np.sqrt(var_pred * var_valid)\n",
    "\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "execution": {
     "iopub.execute_input": "2023-02-19T11:40:19.389021Z",
     "iopub.status.busy": "2023-02-19T11:40:19.387779Z",
     "iopub.status.idle": "2023-02-19T11:40:19.429486Z",
     "shell.execute_reply": "2023-02-19T11:40:19.428403Z",
     "shell.execute_reply.started": "2023-02-19T11:40:19.388979Z"
    },
    "id": "B-9usxYEpJqC",
    "outputId": "37c706cd-3617-438f-9ce0-33cd01b1248b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date_time_x', 'turnoverRatio', 'transactionAmount', 'pe_ttm', 'pe',\n",
       "       'pb', 'ps', 'pcf', 'date_time_y', 'open', 'close', 'high', 'low',\n",
       "       'volume', 'money', 'day', 'time_step', 'day_s', 'code', 'date_time',\n",
       "       'return'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date_time_x', 'turnoverRatio', 'transactionAmount', 'pe_ttm', 'pe',\n",
       "       'pb', 'ps', 'pcf', 'date_time_y', 'open', 'close', 'high', 'low',\n",
       "       'volume', 'money', 'day', 'time_step', 'day_s', 'code'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for normalizing data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#because these time variables are not number, they cannot be scaled\n",
    "col_train=[i for i in df.columns if i not in ['date_time_x', 'date_time_y', 'day', 'time_step', 'day_s', 'code', 'date_time','return']]\n",
    "timer_train=df.loc[:,['date_time_x', 'date_time_y', 'day', 'time_step', 'day_s', 'code', 'date_time','return']]\n",
    "#scale those x variables in training dataset\n",
    "scaled_df=scaler.fit_transform(df[col_train])\n",
    "scaled_df=pd.DataFrame(scaled_df,columns=['turnoverRatio', 'transactionAmount', 'pe_ttm', 'pe', 'pb', 'ps', 'pcf','open', 'close', 'high', 'low', 'volume', 'money'])\n",
    "#add timer back to the df\n",
    "new_df=pd.merge(scaled_df,timer_train,how='outer',left_index=True,right_index=True)\n",
    "\n",
    "#same process for test dataset\n",
    "col_test=[i for i in test.columns if i not in ['date_time_x', 'date_time_y', 'day', 'time_step', 'day_s', 'code', 'date_time']]\n",
    "timer_test=test.loc[:,['date_time_x', 'date_time_y', 'day', 'time_step', 'day_s', 'code']]\n",
    "scaled_test=scaler.fit_transform(test[col_test])\n",
    "scaled_test=pd.DataFrame(scaled_test,columns=['turnoverRatio', 'transactionAmount', 'pe_ttm', 'pe', 'pb', 'ps', 'pcf','open', 'close', 'high', 'low', 'volume', 'money'])\n",
    "new_test=pd.merge(scaled_test,timer_test,how='outer',left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>turnoverRatio</th>\n",
       "      <th>transactionAmount</th>\n",
       "      <th>pe_ttm</th>\n",
       "      <th>pe</th>\n",
       "      <th>pb</th>\n",
       "      <th>ps</th>\n",
       "      <th>pcf</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>volume</th>\n",
       "      <th>money</th>\n",
       "      <th>date_time_x</th>\n",
       "      <th>date_time_y</th>\n",
       "      <th>day</th>\n",
       "      <th>time_step</th>\n",
       "      <th>day_s</th>\n",
       "      <th>code</th>\n",
       "      <th>date_time</th>\n",
       "      <th>return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.093616</td>\n",
       "      <td>0.051792</td>\n",
       "      <td>0.025331</td>\n",
       "      <td>0.046411</td>\n",
       "      <td>0.163548</td>\n",
       "      <td>0.083384</td>\n",
       "      <td>0.948034</td>\n",
       "      <td>0.024360</td>\n",
       "      <td>0.024333</td>\n",
       "      <td>0.024332</td>\n",
       "      <td>0.024362</td>\n",
       "      <td>0.004363</td>\n",
       "      <td>0.009410</td>\n",
       "      <td>s0d1</td>\n",
       "      <td>s0d1p50</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>s0d1</td>\n",
       "      <td>s0</td>\n",
       "      <td>s0d1</td>\n",
       "      <td>-0.026877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.063802</td>\n",
       "      <td>0.011141</td>\n",
       "      <td>0.025208</td>\n",
       "      <td>0.046202</td>\n",
       "      <td>0.168140</td>\n",
       "      <td>0.065664</td>\n",
       "      <td>0.951514</td>\n",
       "      <td>0.015426</td>\n",
       "      <td>0.015374</td>\n",
       "      <td>0.015532</td>\n",
       "      <td>0.015375</td>\n",
       "      <td>0.001816</td>\n",
       "      <td>0.002596</td>\n",
       "      <td>s1d1</td>\n",
       "      <td>s1d1p50</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>s1d1</td>\n",
       "      <td>s1</td>\n",
       "      <td>s1d1</td>\n",
       "      <td>-0.052674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.032330</td>\n",
       "      <td>0.015439</td>\n",
       "      <td>0.025523</td>\n",
       "      <td>0.046811</td>\n",
       "      <td>0.158080</td>\n",
       "      <td>0.090780</td>\n",
       "      <td>0.951040</td>\n",
       "      <td>0.007680</td>\n",
       "      <td>0.007667</td>\n",
       "      <td>0.007667</td>\n",
       "      <td>0.007681</td>\n",
       "      <td>0.002155</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>s2d1</td>\n",
       "      <td>s2d1p50</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>s2d1</td>\n",
       "      <td>s2</td>\n",
       "      <td>s2d1</td>\n",
       "      <td>-0.002691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53889</th>\n",
       "      <td>0.028856</td>\n",
       "      <td>0.015569</td>\n",
       "      <td>0.024882</td>\n",
       "      <td>0.045675</td>\n",
       "      <td>0.019734</td>\n",
       "      <td>0.011614</td>\n",
       "      <td>0.951489</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.001897</td>\n",
       "      <td>0.001897</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.004846</td>\n",
       "      <td>0.001592</td>\n",
       "      <td>s51d998</td>\n",
       "      <td>s51d998p50</td>\n",
       "      <td>998</td>\n",
       "      <td>50</td>\n",
       "      <td>s51d998</td>\n",
       "      <td>s51</td>\n",
       "      <td>s51d998</td>\n",
       "      <td>-0.052286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53890</th>\n",
       "      <td>0.013962</td>\n",
       "      <td>0.022720</td>\n",
       "      <td>0.025208</td>\n",
       "      <td>0.046273</td>\n",
       "      <td>0.197897</td>\n",
       "      <td>0.081012</td>\n",
       "      <td>0.948013</td>\n",
       "      <td>0.029243</td>\n",
       "      <td>0.029194</td>\n",
       "      <td>0.029194</td>\n",
       "      <td>0.029245</td>\n",
       "      <td>0.001526</td>\n",
       "      <td>0.003895</td>\n",
       "      <td>s52d998</td>\n",
       "      <td>s52d998p50</td>\n",
       "      <td>998</td>\n",
       "      <td>50</td>\n",
       "      <td>s52d998</td>\n",
       "      <td>s52</td>\n",
       "      <td>s52d998</td>\n",
       "      <td>-0.015559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53891</th>\n",
       "      <td>0.032522</td>\n",
       "      <td>0.029102</td>\n",
       "      <td>0.024792</td>\n",
       "      <td>0.045511</td>\n",
       "      <td>0.043472</td>\n",
       "      <td>0.004854</td>\n",
       "      <td>0.952153</td>\n",
       "      <td>0.008670</td>\n",
       "      <td>0.008655</td>\n",
       "      <td>0.008655</td>\n",
       "      <td>0.008671</td>\n",
       "      <td>0.004737</td>\n",
       "      <td>0.004162</td>\n",
       "      <td>s53d998</td>\n",
       "      <td>s53d998p50</td>\n",
       "      <td>998</td>\n",
       "      <td>50</td>\n",
       "      <td>s53d998</td>\n",
       "      <td>s53</td>\n",
       "      <td>s53d998</td>\n",
       "      <td>-0.003662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53892 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       turnoverRatio  transactionAmount    pe_ttm        pe        pb  \\\n",
       "0           0.093616           0.051792  0.025331  0.046411  0.163548   \n",
       "1           0.063802           0.011141  0.025208  0.046202  0.168140   \n",
       "2           0.032330           0.015439  0.025523  0.046811  0.158080   \n",
       "...              ...                ...       ...       ...       ...   \n",
       "53889       0.028856           0.015569  0.024882  0.045675  0.019734   \n",
       "53890       0.013962           0.022720  0.025208  0.046273  0.197897   \n",
       "53891       0.032522           0.029102  0.024792  0.045511  0.043472   \n",
       "\n",
       "             ps       pcf      open     close      high       low    volume  \\\n",
       "0      0.083384  0.948034  0.024360  0.024333  0.024332  0.024362  0.004363   \n",
       "1      0.065664  0.951514  0.015426  0.015374  0.015532  0.015375  0.001816   \n",
       "2      0.090780  0.951040  0.007680  0.007667  0.007667  0.007681  0.002155   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "53889  0.011614  0.951489  0.001900  0.001897  0.001897  0.001900  0.004846   \n",
       "53890  0.081012  0.948013  0.029243  0.029194  0.029194  0.029245  0.001526   \n",
       "53891  0.004854  0.952153  0.008670  0.008655  0.008655  0.008671  0.004737   \n",
       "\n",
       "          money date_time_x date_time_y  day time_step    day_s code  \\\n",
       "0      0.009410        s0d1     s0d1p50    1        50     s0d1   s0   \n",
       "1      0.002596        s1d1     s1d1p50    1        50     s1d1   s1   \n",
       "2      0.001721        s2d1     s2d1p50    1        50     s2d1   s2   \n",
       "...         ...         ...         ...  ...       ...      ...  ...   \n",
       "53889  0.001592     s51d998  s51d998p50  998        50  s51d998  s51   \n",
       "53890  0.003895     s52d998  s52d998p50  998        50  s52d998  s52   \n",
       "53891  0.004162     s53d998  s53d998p50  998        50  s53d998  s53   \n",
       "\n",
       "      date_time    return  \n",
       "0          s0d1 -0.026877  \n",
       "1          s1d1 -0.052674  \n",
       "2          s2d1 -0.002691  \n",
       "...         ...       ...  \n",
       "53889   s51d998 -0.052286  \n",
       "53890   s52d998 -0.015559  \n",
       "53891   s53d998 -0.003662  \n",
       "\n",
       "[53892 rows x 21 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T11:42:06.156023Z",
     "iopub.status.busy": "2023-02-19T11:42:06.155626Z",
     "iopub.status.idle": "2023-02-19T11:42:06.169707Z",
     "shell.execute_reply": "2023-02-19T11:42:06.168505Z",
     "shell.execute_reply.started": "2023-02-19T11:42:06.155992Z"
    },
    "id": "jhkIqDnLpJqD"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(train,test):\n",
    "    # Hyperparammeters (just basic)\n",
    "    params = {\n",
    "      'objective': 'rmse',  \n",
    "      'boosting_type': 'gbdt',\n",
    "      'n_jobs': -1,\n",
    "      'verbose': -1\n",
    "    }\n",
    "    \n",
    "    # Split features and target\n",
    "    \n",
    "    x = train[[i for i in df.columns if i not in ['date_time_x', 'date_time_y', 'day', 'time_step', 'day_s', 'code', 'date_time','return']]]\n",
    "    y = train['return']\n",
    "    \n",
    "    x_test = test[[i for i in df.columns if i not in ['date_time_x', 'date_time_y', 'day', 'time_step', 'day_s', 'code', 'date_time','return']]]\n",
    "\n",
    "    oof_predictions = np.zeros(x.shape[0])\n",
    "    test_predictions = np.zeros(x_test.shape[0])\n",
    "    scores = []\n",
    "\n",
    "    # Create a KFold object\n",
    "    gkf = TimeSeriesSplit(n_splits=n_fold,gap=group_gap)\n",
    "    for fold, (trn_ind, val_ind) in enumerate(gkf.split(train['day'].values)):\n",
    "    \n",
    "        print(f'Training fold {fold + 1}')\n",
    "        x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n",
    "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
    "        \n",
    "        #这下面的用到lgb了\n",
    "        train_dataset = lgb.Dataset(x_train, y_train)\n",
    "        val_dataset = lgb.Dataset(x_val, y_val)\n",
    "        model = lgb.train(params = params, \n",
    "                          train_set = train_dataset, \n",
    "                          valid_sets = [train_dataset, val_dataset], \n",
    "                          num_boost_round = 200, \n",
    "                          early_stopping_rounds = 20, \n",
    "                          verbose_eval = False,\n",
    "                          feval = correlation)\n",
    "        # Add predictions to the out of folds array\n",
    "        \n",
    "        oof_predictions[val_ind] = model.predict(x_val)\n",
    "        \n",
    "        rmspe_score = corr_score(y_val,oof_predictions[val_ind])\n",
    "        print(f'Our out of folds corr_score is {rmspe_score}')\n",
    "        scores.append(rmspe_score)\n",
    "        test_predictions += model.predict(x_test) \n",
    "        \n",
    "    rmspe_score = corr_score(y, oof_predictions)\n",
    "    print(scores)\n",
    "    print(f'Our out of folds corr score is {rmspe_score}')\n",
    "    \n",
    "    # Return test predictions\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T11:42:06.156023Z",
     "iopub.status.busy": "2023-02-19T11:42:06.155626Z",
     "iopub.status.idle": "2023-02-19T11:42:06.169707Z",
     "shell.execute_reply": "2023-02-19T11:42:06.168505Z",
     "shell.execute_reply.started": "2023-02-19T11:42:06.155992Z"
    },
    "id": "jhkIqDnLpJqD"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(train,test):\n",
    "    # Hyperparammeters (just basic)\n",
    "    params = {\n",
    "      'objective': 'rmse',  \n",
    "      'boosting_type': 'gbdt',\n",
    "      'n_jobs': -1,\n",
    "      'verbose': -1\n",
    "    }\n",
    "    \n",
    "    # Split features and target\n",
    "    \n",
    "    x = train[[i for i in df.columns if i not in ['date_time_x', 'date_time_y', 'day', 'time_step', \n",
    "                                                  'day_s', 'code', 'date_time','return']]]\n",
    "    y = train['return']\n",
    "    \n",
    "    x_test = test[[i for i in df.columns if i not in ['date_time_x', 'date_time_y', 'day', 'time_step', \n",
    "                                                      'day_s', 'code', 'date_time','return']]]\n",
    "\n",
    "    oof_predictions = np.zeros(x.shape[0])\n",
    "    test_predictions = np.zeros(x_test.shape[0])\n",
    "    scores = []\n",
    "\n",
    "    # Create a KFold object\n",
    "    gkf = TimeSeriesSplit(n_splits=n_fold,gap=group_gap)\n",
    "    for fold, (trn_ind, val_ind) in enumerate(gkf.split(train['day'].values)):\n",
    "    \n",
    "        print(f'Training fold {fold + 1}')\n",
    "        x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n",
    "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
    "        \n",
    "        # create and fit the LSTM network\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(units=64, return_sequences=True,dropout=0.1,recurrent_dropout=0.1,input_shape=(x_train.shape[1],1)))\n",
    "        model.add(LSTM(units=32))\n",
    "        model.add(Dense(1))\n",
    " \n",
    "        model.compile(loss='mean_squared_error', optimizer='adam', metrics='mae')\n",
    "        model.fit(x_train, y_train, epochs=32, batch_size=512, verbose=2)\n",
    "        \n",
    "        #This train_pred must have the same shape as the dataset on which you fitted the scaler. \n",
    "        #To do the inverse_transform you can extract the needed attributes from your scaler \n",
    "        #and apply them to your prediction.\n",
    "        scaler_pred=MinMaxScaler()\n",
    "        scaler_pred.min_, scaler_pred.scale_ = scaler.min_[1], scaler.scale_[1]\n",
    "        \n",
    "        #calculate validation prediction\n",
    "        val_pred=scaler_pred.inverse_transform(model.predict(x_val))\n",
    "        val_pred=np.reshape(val_pred,(val_pred.shape[0],))\n",
    "        oof_predictions[val_ind] = val_pred \n",
    "        rmspe_score = corr_score(y_val,oof_predictions[val_ind])\n",
    "        print(f'Our out of folds corr_score is {rmspe_score}')\n",
    "        scores.append(rmspe_score)\n",
    "        \n",
    "        #calculate test prediction\n",
    "        test_pred=scaler_pred.inverse_transform(model.predict(x_test))\n",
    "        test_pred=np.reshape(test_pred,(test_pred.shape[0],))\n",
    "        test_predictions = test_predictions + test_pred\n",
    "        \n",
    "        #clear session\n",
    "        keras.backend.clear_session()\n",
    "        del model\n",
    "        \n",
    "    rmspe_score = corr_score(y, oof_predictions)\n",
    "    print(scores)\n",
    "    print(f'Our out of folds corr score is {rmspe_score}')\n",
    "    \n",
    "    # Return test predictions\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-02-19T11:42:06.401036Z",
     "iopub.status.busy": "2023-02-19T11:42:06.400583Z",
     "iopub.status.idle": "2023-02-19T11:42:08.132067Z",
     "shell.execute_reply": "2023-02-19T11:42:08.131046Z",
     "shell.execute_reply.started": "2023-02-19T11:42:06.401001Z"
    },
    "id": "s2MYJdoEpJqD",
    "outputId": "bce4f8ed-9413-4f63-8fff-6d27f951369e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "Epoch 1/32\n",
      "10/10 - 5s - loss: 0.0018 - mae: 0.0314 - 5s/epoch - 525ms/step\n",
      "Epoch 2/32\n",
      "10/10 - 1s - loss: 0.0017 - mae: 0.0304 - 766ms/epoch - 77ms/step\n",
      "Epoch 3/32\n",
      "10/10 - 1s - loss: 0.0017 - mae: 0.0302 - 804ms/epoch - 80ms/step\n",
      "Epoch 4/32\n",
      "10/10 - 1s - loss: 0.0017 - mae: 0.0302 - 798ms/epoch - 80ms/step\n",
      "Epoch 5/32\n",
      "10/10 - 1s - loss: 0.0017 - mae: 0.0302 - 756ms/epoch - 76ms/step\n",
      "Epoch 6/32\n",
      "10/10 - 1s - loss: 0.0017 - mae: 0.0303 - 737ms/epoch - 74ms/step\n",
      "Epoch 7/32\n",
      "10/10 - 1s - loss: 0.0017 - mae: 0.0302 - 743ms/epoch - 74ms/step\n",
      "Epoch 8/32\n",
      "10/10 - 1s - loss: 0.0017 - mae: 0.0302 - 740ms/epoch - 74ms/step\n",
      "Epoch 9/32\n",
      "10/10 - 1s - loss: 0.0017 - mae: 0.0302 - 746ms/epoch - 75ms/step\n",
      "Epoch 10/32\n",
      "10/10 - 1s - loss: 0.0017 - mae: 0.0302 - 761ms/epoch - 76ms/step\n",
      "Epoch 11/32\n",
      "10/10 - 1s - loss: 0.0017 - mae: 0.0302 - 763ms/epoch - 76ms/step\n",
      "Epoch 12/32\n",
      "10/10 - 1s - loss: 0.0017 - mae: 0.0301 - 736ms/epoch - 74ms/step\n",
      "Epoch 13/32\n",
      "10/10 - 1s - loss: 0.0017 - mae: 0.0304 - 737ms/epoch - 74ms/step\n",
      "Epoch 14/32\n",
      "10/10 - 1s - loss: 0.0017 - mae: 0.0303 - 746ms/epoch - 75ms/step\n",
      "Epoch 15/32\n",
      "10/10 - 1s - loss: 0.0017 - mae: 0.0305 - 743ms/epoch - 74ms/step\n",
      "Epoch 16/32\n",
      "10/10 - 1s - loss: 0.0017 - mae: 0.0301 - 747ms/epoch - 75ms/step\n",
      "Epoch 17/32\n",
      "10/10 - 1s - loss: 0.0017 - mae: 0.0301 - 752ms/epoch - 75ms/step\n",
      "Epoch 18/32\n",
      "10/10 - 1s - loss: 0.0017 - mae: 0.0302 - 746ms/epoch - 75ms/step\n",
      "Epoch 19/32\n",
      "10/10 - 1s - loss: 0.0017 - mae: 0.0301 - 738ms/epoch - 74ms/step\n",
      "Epoch 20/32\n",
      "10/10 - 1s - loss: 0.0017 - mae: 0.0302 - 748ms/epoch - 75ms/step\n",
      "Epoch 21/32\n",
      "10/10 - 1s - loss: 0.0017 - mae: 0.0302 - 743ms/epoch - 74ms/step\n",
      "Epoch 22/32\n",
      "10/10 - 1s - loss: 0.0017 - mae: 0.0300 - 750ms/epoch - 75ms/step\n",
      "Epoch 23/32\n",
      "10/10 - 1s - loss: 0.0017 - mae: 0.0303 - 748ms/epoch - 75ms/step\n",
      "Epoch 24/32\n",
      "10/10 - 1s - loss: 0.0017 - mae: 0.0304 - 750ms/epoch - 75ms/step\n",
      "Epoch 25/32\n",
      "10/10 - 1s - loss: 0.0017 - mae: 0.0302 - 745ms/epoch - 75ms/step\n",
      "Epoch 26/32\n",
      "10/10 - 1s - loss: 0.0017 - mae: 0.0301 - 745ms/epoch - 75ms/step\n",
      "Epoch 27/32\n",
      "10/10 - 1s - loss: 0.0017 - mae: 0.0302 - 744ms/epoch - 74ms/step\n",
      "Epoch 28/32\n",
      "10/10 - 1s - loss: 0.0017 - mae: 0.0302 - 749ms/epoch - 75ms/step\n",
      "Epoch 29/32\n",
      "10/10 - 1s - loss: 0.0017 - mae: 0.0302 - 745ms/epoch - 74ms/step\n",
      "Epoch 30/32\n",
      "10/10 - 1s - loss: 0.0017 - mae: 0.0301 - 749ms/epoch - 75ms/step\n",
      "Epoch 31/32\n",
      "10/10 - 1s - loss: 0.0017 - mae: 0.0301 - 757ms/epoch - 76ms/step\n",
      "Epoch 32/32\n",
      "10/10 - 1s - loss: 0.0017 - mae: 0.0301 - 758ms/epoch - 76ms/step\n",
      "Our out of folds corr_score is 0.022070941829676012\n",
      "Training fold 2\n",
      "Epoch 1/32\n",
      "20/20 - 8s - loss: 0.0047 - mae: 0.0491 - 8s/epoch - 391ms/step\n",
      "Epoch 2/32\n",
      "20/20 - 3s - loss: 0.0046 - mae: 0.0483 - 3s/epoch - 142ms/step\n",
      "Epoch 3/32\n",
      "20/20 - 3s - loss: 0.0046 - mae: 0.0484 - 3s/epoch - 151ms/step\n",
      "Epoch 4/32\n",
      "20/20 - 3s - loss: 0.0046 - mae: 0.0483 - 3s/epoch - 148ms/step\n",
      "Epoch 5/32\n",
      "20/20 - 3s - loss: 0.0046 - mae: 0.0485 - 3s/epoch - 143ms/step\n",
      "Epoch 6/32\n",
      "20/20 - 3s - loss: 0.0046 - mae: 0.0483 - 3s/epoch - 144ms/step\n",
      "Epoch 7/32\n",
      "20/20 - 3s - loss: 0.0046 - mae: 0.0483 - 3s/epoch - 142ms/step\n",
      "Epoch 8/32\n",
      "20/20 - 3s - loss: 0.0046 - mae: 0.0484 - 3s/epoch - 144ms/step\n",
      "Epoch 9/32\n",
      "20/20 - 3s - loss: 0.0046 - mae: 0.0482 - 3s/epoch - 143ms/step\n",
      "Epoch 10/32\n",
      "20/20 - 3s - loss: 0.0046 - mae: 0.0482 - 3s/epoch - 143ms/step\n",
      "Epoch 11/32\n",
      "20/20 - 3s - loss: 0.0046 - mae: 0.0482 - 3s/epoch - 144ms/step\n",
      "Epoch 12/32\n",
      "20/20 - 3s - loss: 0.0046 - mae: 0.0483 - 3s/epoch - 144ms/step\n",
      "Epoch 13/32\n",
      "20/20 - 3s - loss: 0.0046 - mae: 0.0484 - 3s/epoch - 144ms/step\n",
      "Epoch 14/32\n",
      "20/20 - 3s - loss: 0.0046 - mae: 0.0483 - 3s/epoch - 144ms/step\n",
      "Epoch 15/32\n",
      "20/20 - 3s - loss: 0.0046 - mae: 0.0485 - 3s/epoch - 143ms/step\n",
      "Epoch 16/32\n",
      "20/20 - 3s - loss: 0.0046 - mae: 0.0483 - 3s/epoch - 145ms/step\n",
      "Epoch 17/32\n",
      "20/20 - 3s - loss: 0.0046 - mae: 0.0483 - 3s/epoch - 143ms/step\n",
      "Epoch 18/32\n",
      "20/20 - 3s - loss: 0.0046 - mae: 0.0482 - 3s/epoch - 144ms/step\n",
      "Epoch 19/32\n",
      "20/20 - 3s - loss: 0.0046 - mae: 0.0482 - 3s/epoch - 144ms/step\n",
      "Epoch 20/32\n",
      "20/20 - 3s - loss: 0.0046 - mae: 0.0482 - 3s/epoch - 145ms/step\n",
      "Epoch 21/32\n",
      "20/20 - 3s - loss: 0.0046 - mae: 0.0483 - 3s/epoch - 146ms/step\n",
      "Epoch 22/32\n",
      "20/20 - 3s - loss: 0.0046 - mae: 0.0483 - 3s/epoch - 145ms/step\n",
      "Epoch 23/32\n",
      "20/20 - 3s - loss: 0.0046 - mae: 0.0484 - 3s/epoch - 146ms/step\n",
      "Epoch 24/32\n",
      "20/20 - 3s - loss: 0.0046 - mae: 0.0482 - 3s/epoch - 147ms/step\n",
      "Epoch 25/32\n",
      "20/20 - 3s - loss: 0.0046 - mae: 0.0482 - 3s/epoch - 147ms/step\n",
      "Epoch 26/32\n",
      "20/20 - 3s - loss: 0.0046 - mae: 0.0483 - 3s/epoch - 148ms/step\n",
      "Epoch 27/32\n",
      "20/20 - 3s - loss: 0.0046 - mae: 0.0483 - 3s/epoch - 153ms/step\n",
      "Epoch 28/32\n",
      "20/20 - 3s - loss: 0.0046 - mae: 0.0484 - 3s/epoch - 150ms/step\n",
      "Epoch 29/32\n",
      "20/20 - 3s - loss: 0.0046 - mae: 0.0483 - 3s/epoch - 150ms/step\n",
      "Epoch 30/32\n",
      "20/20 - 3s - loss: 0.0046 - mae: 0.0482 - 3s/epoch - 151ms/step\n",
      "Epoch 31/32\n",
      "20/20 - 3s - loss: 0.0046 - mae: 0.0482 - 3s/epoch - 152ms/step\n",
      "Epoch 32/32\n",
      "20/20 - 3s - loss: 0.0046 - mae: 0.0482 - 3s/epoch - 155ms/step\n",
      "Our out of folds corr_score is 0.032396583387459564\n",
      "Training fold 3\n",
      "Epoch 1/32\n",
      "29/29 - 8s - loss: 0.0038 - mae: 0.0446 - 8s/epoch - 273ms/step\n",
      "Epoch 2/32\n",
      "29/29 - 5s - loss: 0.0038 - mae: 0.0439 - 5s/epoch - 166ms/step\n",
      "Epoch 3/32\n",
      "29/29 - 5s - loss: 0.0038 - mae: 0.0438 - 5s/epoch - 171ms/step\n",
      "Epoch 4/32\n",
      "29/29 - 5s - loss: 0.0038 - mae: 0.0439 - 5s/epoch - 172ms/step\n",
      "Epoch 5/32\n",
      "29/29 - 5s - loss: 0.0038 - mae: 0.0440 - 5s/epoch - 177ms/step\n",
      "Epoch 6/32\n",
      "29/29 - 6s - loss: 0.0038 - mae: 0.0438 - 6s/epoch - 190ms/step\n",
      "Epoch 7/32\n",
      "29/29 - 5s - loss: 0.0038 - mae: 0.0439 - 5s/epoch - 185ms/step\n",
      "Epoch 8/32\n",
      "29/29 - 5s - loss: 0.0038 - mae: 0.0438 - 5s/epoch - 183ms/step\n",
      "Epoch 9/32\n",
      "29/29 - 5s - loss: 0.0038 - mae: 0.0438 - 5s/epoch - 185ms/step\n",
      "Epoch 10/32\n",
      "29/29 - 5s - loss: 0.0038 - mae: 0.0438 - 5s/epoch - 188ms/step\n",
      "Epoch 11/32\n",
      "29/29 - 5s - loss: 0.0038 - mae: 0.0439 - 5s/epoch - 189ms/step\n",
      "Epoch 12/32\n",
      "29/29 - 6s - loss: 0.0038 - mae: 0.0438 - 6s/epoch - 212ms/step\n",
      "Epoch 13/32\n",
      "29/29 - 6s - loss: 0.0038 - mae: 0.0439 - 6s/epoch - 222ms/step\n",
      "Epoch 14/32\n",
      "29/29 - 7s - loss: 0.0038 - mae: 0.0438 - 7s/epoch - 225ms/step\n",
      "Epoch 15/32\n",
      "29/29 - 7s - loss: 0.0038 - mae: 0.0438 - 7s/epoch - 239ms/step\n",
      "Epoch 16/32\n",
      "29/29 - 7s - loss: 0.0038 - mae: 0.0438 - 7s/epoch - 237ms/step\n",
      "Epoch 17/32\n",
      "29/29 - 6s - loss: 0.0038 - mae: 0.0437 - 6s/epoch - 207ms/step\n",
      "Epoch 18/32\n",
      "29/29 - 6s - loss: 0.0038 - mae: 0.0438 - 6s/epoch - 198ms/step\n",
      "Epoch 19/32\n",
      "29/29 - 6s - loss: 0.0038 - mae: 0.0437 - 6s/epoch - 209ms/step\n",
      "Epoch 20/32\n",
      "29/29 - 6s - loss: 0.0038 - mae: 0.0438 - 6s/epoch - 197ms/step\n",
      "Epoch 21/32\n",
      "29/29 - 6s - loss: 0.0038 - mae: 0.0437 - 6s/epoch - 196ms/step\n",
      "Epoch 22/32\n",
      "29/29 - 5s - loss: 0.0038 - mae: 0.0438 - 5s/epoch - 189ms/step\n",
      "Epoch 23/32\n",
      "29/29 - 5s - loss: 0.0038 - mae: 0.0437 - 5s/epoch - 184ms/step\n",
      "Epoch 24/32\n",
      "29/29 - 6s - loss: 0.0038 - mae: 0.0438 - 6s/epoch - 198ms/step\n",
      "Epoch 25/32\n",
      "29/29 - 6s - loss: 0.0038 - mae: 0.0438 - 6s/epoch - 191ms/step\n",
      "Epoch 26/32\n",
      "29/29 - 5s - loss: 0.0038 - mae: 0.0438 - 5s/epoch - 183ms/step\n",
      "Epoch 27/32\n",
      "29/29 - 5s - loss: 0.0038 - mae: 0.0438 - 5s/epoch - 185ms/step\n",
      "Epoch 28/32\n",
      "29/29 - 5s - loss: 0.0038 - mae: 0.0438 - 5s/epoch - 183ms/step\n",
      "Epoch 29/32\n",
      "29/29 - 5s - loss: 0.0038 - mae: 0.0438 - 5s/epoch - 180ms/step\n",
      "Epoch 30/32\n",
      "29/29 - 5s - loss: 0.0038 - mae: 0.0438 - 5s/epoch - 177ms/step\n",
      "Epoch 31/32\n",
      "29/29 - 5s - loss: 0.0038 - mae: 0.0438 - 5s/epoch - 179ms/step\n",
      "Epoch 32/32\n",
      "29/29 - 5s - loss: 0.0038 - mae: 0.0439 - 5s/epoch - 174ms/step\n",
      "Our out of folds corr_score is 0.07464164040330175\n",
      "Training fold 4\n",
      "Epoch 1/32\n",
      "39/39 - 14s - loss: 0.0033 - mae: 0.0406 - 14s/epoch - 356ms/step\n",
      "Epoch 2/32\n",
      "39/39 - 8s - loss: 0.0032 - mae: 0.0400 - 8s/epoch - 211ms/step\n",
      "Epoch 3/32\n",
      "39/39 - 9s - loss: 0.0032 - mae: 0.0401 - 9s/epoch - 224ms/step\n",
      "Epoch 4/32\n",
      "39/39 - 9s - loss: 0.0032 - mae: 0.0402 - 9s/epoch - 225ms/step\n",
      "Epoch 5/32\n",
      "39/39 - 9s - loss: 0.0032 - mae: 0.0401 - 9s/epoch - 225ms/step\n",
      "Epoch 6/32\n",
      "39/39 - 8s - loss: 0.0032 - mae: 0.0401 - 8s/epoch - 218ms/step\n",
      "Epoch 7/32\n",
      "39/39 - 8s - loss: 0.0032 - mae: 0.0400 - 8s/epoch - 211ms/step\n",
      "Epoch 8/32\n",
      "39/39 - 8s - loss: 0.0032 - mae: 0.0400 - 8s/epoch - 206ms/step\n",
      "Epoch 9/32\n",
      "39/39 - 8s - loss: 0.0032 - mae: 0.0400 - 8s/epoch - 202ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/32\n",
      "39/39 - 7s - loss: 0.0032 - mae: 0.0400 - 7s/epoch - 185ms/step\n",
      "Epoch 11/32\n",
      "39/39 - 7s - loss: 0.0032 - mae: 0.0401 - 7s/epoch - 183ms/step\n",
      "Epoch 12/32\n",
      "39/39 - 7s - loss: 0.0032 - mae: 0.0401 - 7s/epoch - 182ms/step\n",
      "Epoch 13/32\n",
      "39/39 - 7s - loss: 0.0032 - mae: 0.0400 - 7s/epoch - 180ms/step\n",
      "Epoch 14/32\n",
      "39/39 - 7s - loss: 0.0032 - mae: 0.0400 - 7s/epoch - 185ms/step\n",
      "Epoch 15/32\n",
      "39/39 - 9s - loss: 0.0032 - mae: 0.0400 - 9s/epoch - 224ms/step\n",
      "Epoch 16/32\n",
      "39/39 - 8s - loss: 0.0032 - mae: 0.0400 - 8s/epoch - 203ms/step\n",
      "Epoch 17/32\n",
      "39/39 - 7s - loss: 0.0032 - mae: 0.0400 - 7s/epoch - 182ms/step\n",
      "Epoch 18/32\n",
      "39/39 - 7s - loss: 0.0032 - mae: 0.0400 - 7s/epoch - 186ms/step\n",
      "Epoch 19/32\n",
      "39/39 - 8s - loss: 0.0032 - mae: 0.0400 - 8s/epoch - 193ms/step\n",
      "Epoch 20/32\n",
      "39/39 - 9s - loss: 0.0032 - mae: 0.0400 - 9s/epoch - 222ms/step\n",
      "Epoch 21/32\n",
      "39/39 - 9s - loss: 0.0032 - mae: 0.0400 - 9s/epoch - 227ms/step\n",
      "Epoch 22/32\n",
      "39/39 - 8s - loss: 0.0032 - mae: 0.0400 - 8s/epoch - 216ms/step\n",
      "Epoch 23/32\n",
      "39/39 - 8s - loss: 0.0032 - mae: 0.0399 - 8s/epoch - 202ms/step\n",
      "Epoch 24/32\n",
      "39/39 - 8s - loss: 0.0032 - mae: 0.0400 - 8s/epoch - 199ms/step\n",
      "Epoch 25/32\n",
      "39/39 - 8s - loss: 0.0032 - mae: 0.0400 - 8s/epoch - 200ms/step\n",
      "Epoch 26/32\n",
      "39/39 - 8s - loss: 0.0032 - mae: 0.0400 - 8s/epoch - 201ms/step\n",
      "Epoch 27/32\n",
      "39/39 - 9s - loss: 0.0032 - mae: 0.0401 - 9s/epoch - 222ms/step\n",
      "Epoch 28/32\n",
      "39/39 - 8s - loss: 0.0032 - mae: 0.0400 - 8s/epoch - 215ms/step\n",
      "Epoch 29/32\n",
      "39/39 - 8s - loss: 0.0032 - mae: 0.0400 - 8s/epoch - 213ms/step\n",
      "Epoch 30/32\n",
      "39/39 - 8s - loss: 0.0032 - mae: 0.0400 - 8s/epoch - 212ms/step\n",
      "Epoch 31/32\n",
      "39/39 - 8s - loss: 0.0032 - mae: 0.0400 - 8s/epoch - 214ms/step\n",
      "Epoch 32/32\n",
      "39/39 - 8s - loss: 0.0032 - mae: 0.0400 - 8s/epoch - 214ms/step\n",
      "Our out of folds corr_score is 0.0741484592548737\n",
      "Training fold 5\n",
      "Epoch 1/32\n",
      "48/48 - 12s - loss: 0.0027 - mae: 0.0359 - 12s/epoch - 248ms/step\n",
      "Epoch 2/32\n",
      "48/48 - 8s - loss: 0.0027 - mae: 0.0358 - 8s/epoch - 169ms/step\n",
      "Epoch 3/32\n",
      "48/48 - 8s - loss: 0.0027 - mae: 0.0359 - 8s/epoch - 166ms/step\n",
      "Epoch 4/32\n",
      "48/48 - 8s - loss: 0.0027 - mae: 0.0357 - 8s/epoch - 157ms/step\n",
      "Epoch 5/32\n",
      "48/48 - 8s - loss: 0.0027 - mae: 0.0357 - 8s/epoch - 174ms/step\n",
      "Epoch 6/32\n",
      "48/48 - 8s - loss: 0.0027 - mae: 0.0357 - 8s/epoch - 157ms/step\n",
      "Epoch 7/32\n",
      "48/48 - 7s - loss: 0.0027 - mae: 0.0357 - 7s/epoch - 151ms/step\n",
      "Epoch 8/32\n",
      "48/48 - 7s - loss: 0.0027 - mae: 0.0357 - 7s/epoch - 149ms/step\n",
      "Epoch 9/32\n",
      "48/48 - 7s - loss: 0.0027 - mae: 0.0357 - 7s/epoch - 147ms/step\n",
      "Epoch 10/32\n",
      "48/48 - 7s - loss: 0.0027 - mae: 0.0356 - 7s/epoch - 144ms/step\n",
      "Epoch 11/32\n",
      "48/48 - 7s - loss: 0.0027 - mae: 0.0357 - 7s/epoch - 143ms/step\n",
      "Epoch 12/32\n",
      "48/48 - 7s - loss: 0.0027 - mae: 0.0357 - 7s/epoch - 141ms/step\n",
      "Epoch 13/32\n",
      "48/48 - 7s - loss: 0.0027 - mae: 0.0356 - 7s/epoch - 143ms/step\n",
      "Epoch 14/32\n",
      "48/48 - 7s - loss: 0.0027 - mae: 0.0356 - 7s/epoch - 141ms/step\n",
      "Epoch 15/32\n",
      "48/48 - 7s - loss: 0.0027 - mae: 0.0356 - 7s/epoch - 143ms/step\n",
      "Epoch 16/32\n",
      "48/48 - 7s - loss: 0.0027 - mae: 0.0356 - 7s/epoch - 140ms/step\n",
      "Epoch 17/32\n",
      "48/48 - 7s - loss: 0.0027 - mae: 0.0356 - 7s/epoch - 142ms/step\n",
      "Epoch 18/32\n",
      "48/48 - 7s - loss: 0.0027 - mae: 0.0356 - 7s/epoch - 136ms/step\n",
      "Epoch 19/32\n",
      "48/48 - 7s - loss: 0.0027 - mae: 0.0357 - 7s/epoch - 137ms/step\n",
      "Epoch 20/32\n",
      "48/48 - 7s - loss: 0.0027 - mae: 0.0357 - 7s/epoch - 136ms/step\n",
      "Epoch 21/32\n",
      "48/48 - 7s - loss: 0.0027 - mae: 0.0356 - 7s/epoch - 136ms/step\n",
      "Epoch 22/32\n",
      "48/48 - 7s - loss: 0.0027 - mae: 0.0356 - 7s/epoch - 137ms/step\n",
      "Epoch 23/32\n",
      "48/48 - 7s - loss: 0.0027 - mae: 0.0357 - 7s/epoch - 136ms/step\n",
      "Epoch 24/32\n",
      "48/48 - 7s - loss: 0.0027 - mae: 0.0357 - 7s/epoch - 136ms/step\n",
      "Epoch 25/32\n",
      "48/48 - 7s - loss: 0.0027 - mae: 0.0356 - 7s/epoch - 137ms/step\n",
      "Epoch 26/32\n",
      "48/48 - 7s - loss: 0.0027 - mae: 0.0357 - 7s/epoch - 154ms/step\n",
      "Epoch 27/32\n",
      "48/48 - 8s - loss: 0.0027 - mae: 0.0356 - 8s/epoch - 163ms/step\n",
      "Epoch 28/32\n",
      "48/48 - 7s - loss: 0.0027 - mae: 0.0356 - 7s/epoch - 150ms/step\n",
      "Epoch 29/32\n",
      "48/48 - 7s - loss: 0.0027 - mae: 0.0357 - 7s/epoch - 144ms/step\n",
      "Epoch 30/32\n",
      "48/48 - 7s - loss: 0.0027 - mae: 0.0356 - 7s/epoch - 145ms/step\n",
      "Epoch 31/32\n",
      "48/48 - 7s - loss: 0.0027 - mae: 0.0356 - 7s/epoch - 146ms/step\n",
      "Epoch 32/32\n",
      "48/48 - 7s - loss: 0.0027 - mae: 0.0356 - 7s/epoch - 147ms/step\n",
      "Our out of folds corr_score is 0.034470405664758084\n",
      "Training fold 6\n",
      "Epoch 1/32\n",
      "58/58 - 11s - loss: 0.0024 - mae: 0.0332 - 11s/epoch - 187ms/step\n",
      "Epoch 2/32\n",
      "58/58 - 8s - loss: 0.0023 - mae: 0.0326 - 8s/epoch - 132ms/step\n",
      "Epoch 3/32\n",
      "58/58 - 8s - loss: 0.0024 - mae: 0.0328 - 8s/epoch - 132ms/step\n",
      "Epoch 4/32\n",
      "58/58 - 8s - loss: 0.0023 - mae: 0.0326 - 8s/epoch - 141ms/step\n",
      "Epoch 5/32\n",
      "58/58 - 8s - loss: 0.0023 - mae: 0.0326 - 8s/epoch - 141ms/step\n",
      "Epoch 6/32\n",
      "58/58 - 8s - loss: 0.0023 - mae: 0.0326 - 8s/epoch - 145ms/step\n",
      "Epoch 7/32\n",
      "58/58 - 8s - loss: 0.0023 - mae: 0.0326 - 8s/epoch - 140ms/step\n",
      "Epoch 8/32\n",
      "58/58 - 8s - loss: 0.0023 - mae: 0.0326 - 8s/epoch - 142ms/step\n",
      "Epoch 9/32\n",
      "58/58 - 8s - loss: 0.0023 - mae: 0.0326 - 8s/epoch - 144ms/step\n",
      "Epoch 10/32\n",
      "58/58 - 8s - loss: 0.0024 - mae: 0.0327 - 8s/epoch - 143ms/step\n",
      "Epoch 11/32\n",
      "58/58 - 9s - loss: 0.0023 - mae: 0.0326 - 9s/epoch - 154ms/step\n",
      "Epoch 12/32\n",
      "58/58 - 10s - loss: 0.0023 - mae: 0.0326 - 10s/epoch - 164ms/step\n",
      "Epoch 13/32\n",
      "58/58 - 10s - loss: 0.0023 - mae: 0.0326 - 10s/epoch - 168ms/step\n",
      "Epoch 14/32\n",
      "58/58 - 10s - loss: 0.0023 - mae: 0.0326 - 10s/epoch - 168ms/step\n",
      "Epoch 15/32\n",
      "58/58 - 10s - loss: 0.0023 - mae: 0.0326 - 10s/epoch - 172ms/step\n",
      "Epoch 16/32\n",
      "58/58 - 10s - loss: 0.0023 - mae: 0.0326 - 10s/epoch - 173ms/step\n",
      "Epoch 17/32\n",
      "58/58 - 10s - loss: 0.0023 - mae: 0.0326 - 10s/epoch - 170ms/step\n",
      "Epoch 18/32\n",
      "58/58 - 10s - loss: 0.0023 - mae: 0.0326 - 10s/epoch - 171ms/step\n",
      "Epoch 19/32\n",
      "58/58 - 10s - loss: 0.0023 - mae: 0.0326 - 10s/epoch - 175ms/step\n",
      "Epoch 20/32\n",
      "58/58 - 10s - loss: 0.0023 - mae: 0.0326 - 10s/epoch - 173ms/step\n",
      "Epoch 21/32\n",
      "58/58 - 10s - loss: 0.0023 - mae: 0.0326 - 10s/epoch - 172ms/step\n",
      "Epoch 22/32\n",
      "58/58 - 10s - loss: 0.0023 - mae: 0.0327 - 10s/epoch - 170ms/step\n",
      "Epoch 23/32\n",
      "58/58 - 13s - loss: 0.0023 - mae: 0.0326 - 13s/epoch - 222ms/step\n",
      "Epoch 24/32\n",
      "58/58 - 13s - loss: 0.0023 - mae: 0.0326 - 13s/epoch - 216ms/step\n",
      "Epoch 25/32\n",
      "58/58 - 12s - loss: 0.0023 - mae: 0.0326 - 12s/epoch - 201ms/step\n",
      "Epoch 26/32\n",
      "58/58 - 11s - loss: 0.0023 - mae: 0.0326 - 11s/epoch - 191ms/step\n",
      "Epoch 27/32\n",
      "58/58 - 11s - loss: 0.0023 - mae: 0.0326 - 11s/epoch - 195ms/step\n",
      "Epoch 28/32\n",
      "58/58 - 11s - loss: 0.0023 - mae: 0.0326 - 11s/epoch - 182ms/step\n",
      "Epoch 29/32\n",
      "58/58 - 11s - loss: 0.0023 - mae: 0.0326 - 11s/epoch - 185ms/step\n",
      "Epoch 30/32\n",
      "58/58 - 10s - loss: 0.0023 - mae: 0.0326 - 10s/epoch - 172ms/step\n",
      "Epoch 31/32\n",
      "58/58 - 9s - loss: 0.0023 - mae: 0.0326 - 9s/epoch - 153ms/step\n",
      "Epoch 32/32\n",
      "58/58 - 9s - loss: 0.0023 - mae: 0.0326 - 9s/epoch - 150ms/step\n",
      "Our out of folds corr_score is -0.06652206518969175\n",
      "Training fold 7\n",
      "Epoch 1/32\n",
      "67/67 - 15s - loss: 0.0022 - mae: 0.0315 - 15s/epoch - 218ms/step\n",
      "Epoch 2/32\n",
      "67/67 - 10s - loss: 0.0021 - mae: 0.0312 - 10s/epoch - 157ms/step\n",
      "Epoch 3/32\n",
      "67/67 - 10s - loss: 0.0021 - mae: 0.0311 - 10s/epoch - 154ms/step\n",
      "Epoch 4/32\n",
      "67/67 - 10s - loss: 0.0021 - mae: 0.0311 - 10s/epoch - 151ms/step\n",
      "Epoch 5/32\n",
      "67/67 - 10s - loss: 0.0021 - mae: 0.0311 - 10s/epoch - 154ms/step\n",
      "Epoch 6/32\n",
      "67/67 - 10s - loss: 0.0021 - mae: 0.0311 - 10s/epoch - 152ms/step\n",
      "Epoch 7/32\n",
      "67/67 - 12s - loss: 0.0021 - mae: 0.0311 - 12s/epoch - 181ms/step\n",
      "Epoch 8/32\n",
      "67/67 - 13s - loss: 0.0021 - mae: 0.0311 - 13s/epoch - 189ms/step\n",
      "Epoch 9/32\n",
      "67/67 - 11s - loss: 0.0021 - mae: 0.0312 - 11s/epoch - 171ms/step\n",
      "Epoch 10/32\n",
      "67/67 - 11s - loss: 0.0021 - mae: 0.0311 - 11s/epoch - 164ms/step\n",
      "Epoch 11/32\n",
      "67/67 - 11s - loss: 0.0021 - mae: 0.0312 - 11s/epoch - 164ms/step\n",
      "Epoch 12/32\n",
      "67/67 - 11s - loss: 0.0021 - mae: 0.0311 - 11s/epoch - 165ms/step\n",
      "Epoch 13/32\n",
      "67/67 - 11s - loss: 0.0021 - mae: 0.0312 - 11s/epoch - 165ms/step\n",
      "Epoch 14/32\n",
      "67/67 - 12s - loss: 0.0021 - mae: 0.0311 - 12s/epoch - 176ms/step\n",
      "Epoch 15/32\n",
      "67/67 - 12s - loss: 0.0021 - mae: 0.0311 - 12s/epoch - 174ms/step\n",
      "Epoch 16/32\n",
      "67/67 - 19s - loss: 0.0021 - mae: 0.0311 - 19s/epoch - 286ms/step\n",
      "Epoch 17/32\n",
      "67/67 - 11s - loss: 0.0021 - mae: 0.0311 - 11s/epoch - 169ms/step\n",
      "Epoch 18/32\n",
      "67/67 - 11s - loss: 0.0021 - mae: 0.0311 - 11s/epoch - 165ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/32\n",
      "67/67 - 11s - loss: 0.0021 - mae: 0.0311 - 11s/epoch - 157ms/step\n",
      "Epoch 20/32\n",
      "67/67 - 10s - loss: 0.0021 - mae: 0.0311 - 10s/epoch - 149ms/step\n",
      "Epoch 21/32\n",
      "67/67 - 11s - loss: 0.0021 - mae: 0.0311 - 11s/epoch - 162ms/step\n",
      "Epoch 22/32\n",
      "67/67 - 16s - loss: 0.0021 - mae: 0.0311 - 16s/epoch - 239ms/step\n",
      "Epoch 23/32\n",
      "67/67 - 11s - loss: 0.0021 - mae: 0.0311 - 11s/epoch - 171ms/step\n",
      "Epoch 24/32\n",
      "67/67 - 14s - loss: 0.0021 - mae: 0.0311 - 14s/epoch - 216ms/step\n",
      "Epoch 25/32\n",
      "67/67 - 13s - loss: 0.0021 - mae: 0.0311 - 13s/epoch - 192ms/step\n",
      "Epoch 26/32\n",
      "67/67 - 12s - loss: 0.0021 - mae: 0.0311 - 12s/epoch - 175ms/step\n",
      "Epoch 27/32\n",
      "67/67 - 12s - loss: 0.0021 - mae: 0.0311 - 12s/epoch - 179ms/step\n",
      "Epoch 28/32\n",
      "67/67 - 11s - loss: 0.0021 - mae: 0.0311 - 11s/epoch - 166ms/step\n",
      "Epoch 29/32\n",
      "67/67 - 11s - loss: 0.0021 - mae: 0.0311 - 11s/epoch - 160ms/step\n",
      "Epoch 30/32\n",
      "67/67 - 11s - loss: 0.0021 - mae: 0.0311 - 11s/epoch - 163ms/step\n",
      "Epoch 31/32\n",
      "67/67 - 11s - loss: 0.0021 - mae: 0.0311 - 11s/epoch - 164ms/step\n",
      "Epoch 32/32\n",
      "67/67 - 13s - loss: 0.0021 - mae: 0.0311 - 13s/epoch - 195ms/step\n",
      "Our out of folds corr_score is 0.0066864535797939734\n",
      "Training fold 8\n",
      "Epoch 1/32\n",
      "77/77 - 23s - loss: 0.0020 - mae: 0.0303 - 23s/epoch - 297ms/step\n",
      "Epoch 2/32\n",
      "77/77 - 13s - loss: 0.0020 - mae: 0.0300 - 13s/epoch - 175ms/step\n",
      "Epoch 3/32\n",
      "77/77 - 14s - loss: 0.0020 - mae: 0.0299 - 14s/epoch - 181ms/step\n",
      "Epoch 4/32\n",
      "77/77 - 14s - loss: 0.0020 - mae: 0.0299 - 14s/epoch - 184ms/step\n",
      "Epoch 5/32\n",
      "77/77 - 15s - loss: 0.0020 - mae: 0.0299 - 15s/epoch - 199ms/step\n",
      "Epoch 6/32\n",
      "77/77 - 13s - loss: 0.0020 - mae: 0.0299 - 13s/epoch - 164ms/step\n",
      "Epoch 7/32\n",
      "77/77 - 15s - loss: 0.0020 - mae: 0.0299 - 15s/epoch - 189ms/step\n",
      "Epoch 8/32\n",
      "77/77 - 13s - loss: 0.0020 - mae: 0.0299 - 13s/epoch - 171ms/step\n",
      "Epoch 9/32\n",
      "77/77 - 13s - loss: 0.0020 - mae: 0.0299 - 13s/epoch - 172ms/step\n",
      "Epoch 10/32\n",
      "77/77 - 13s - loss: 0.0020 - mae: 0.0299 - 13s/epoch - 174ms/step\n",
      "Epoch 11/32\n",
      "77/77 - 13s - loss: 0.0020 - mae: 0.0299 - 13s/epoch - 173ms/step\n",
      "Epoch 12/32\n",
      "77/77 - 15s - loss: 0.0020 - mae: 0.0299 - 15s/epoch - 198ms/step\n",
      "Epoch 13/32\n",
      "77/77 - 13s - loss: 0.0020 - mae: 0.0299 - 13s/epoch - 167ms/step\n",
      "Epoch 14/32\n",
      "77/77 - 12s - loss: 0.0020 - mae: 0.0299 - 12s/epoch - 162ms/step\n",
      "Epoch 15/32\n",
      "77/77 - 13s - loss: 0.0020 - mae: 0.0299 - 13s/epoch - 169ms/step\n",
      "Epoch 16/32\n",
      "77/77 - 17s - loss: 0.0020 - mae: 0.0299 - 17s/epoch - 215ms/step\n",
      "Epoch 17/32\n",
      "77/77 - 15s - loss: 0.0020 - mae: 0.0299 - 15s/epoch - 193ms/step\n",
      "Epoch 18/32\n",
      "77/77 - 13s - loss: 0.0020 - mae: 0.0299 - 13s/epoch - 166ms/step\n",
      "Epoch 19/32\n",
      "77/77 - 13s - loss: 0.0020 - mae: 0.0299 - 13s/epoch - 168ms/step\n",
      "Epoch 20/32\n",
      "77/77 - 13s - loss: 0.0020 - mae: 0.0299 - 13s/epoch - 163ms/step\n",
      "Epoch 21/32\n",
      "77/77 - 16s - loss: 0.0020 - mae: 0.0298 - 16s/epoch - 208ms/step\n",
      "Epoch 22/32\n",
      "77/77 - 14s - loss: 0.0020 - mae: 0.0299 - 14s/epoch - 180ms/step\n",
      "Epoch 23/32\n",
      "77/77 - 12s - loss: 0.0020 - mae: 0.0299 - 12s/epoch - 160ms/step\n",
      "Epoch 24/32\n",
      "77/77 - 12s - loss: 0.0020 - mae: 0.0298 - 12s/epoch - 158ms/step\n",
      "Epoch 25/32\n",
      "77/77 - 12s - loss: 0.0020 - mae: 0.0299 - 12s/epoch - 159ms/step\n",
      "Epoch 26/32\n",
      "77/77 - 14s - loss: 0.0020 - mae: 0.0298 - 14s/epoch - 180ms/step\n",
      "Epoch 27/32\n",
      "77/77 - 14s - loss: 0.0020 - mae: 0.0298 - 14s/epoch - 177ms/step\n",
      "Epoch 28/32\n",
      "77/77 - 12s - loss: 0.0020 - mae: 0.0298 - 12s/epoch - 162ms/step\n",
      "Epoch 29/32\n",
      "77/77 - 13s - loss: 0.0020 - mae: 0.0298 - 13s/epoch - 163ms/step\n",
      "Epoch 30/32\n",
      "77/77 - 14s - loss: 0.0020 - mae: 0.0299 - 14s/epoch - 181ms/step\n",
      "Epoch 31/32\n",
      "77/77 - 15s - loss: 0.0020 - mae: 0.0299 - 15s/epoch - 189ms/step\n",
      "Epoch 32/32\n",
      "77/77 - 14s - loss: 0.0020 - mae: 0.0299 - 14s/epoch - 183ms/step\n",
      "Our out of folds corr_score is 0.009822748663190462\n",
      "Training fold 9\n",
      "Epoch 1/32\n",
      "87/87 - 23s - loss: 0.0019 - mae: 0.0295 - 23s/epoch - 260ms/step\n",
      "Epoch 2/32\n",
      "87/87 - 21s - loss: 0.0019 - mae: 0.0293 - 21s/epoch - 246ms/step\n",
      "Epoch 3/32\n",
      "87/87 - 19s - loss: 0.0019 - mae: 0.0293 - 19s/epoch - 221ms/step\n",
      "Epoch 4/32\n",
      "87/87 - 19s - loss: 0.0019 - mae: 0.0294 - 19s/epoch - 216ms/step\n",
      "Epoch 5/32\n",
      "87/87 - 21s - loss: 0.0019 - mae: 0.0293 - 21s/epoch - 238ms/step\n",
      "Epoch 6/32\n",
      "87/87 - 20s - loss: 0.0019 - mae: 0.0293 - 20s/epoch - 226ms/step\n",
      "Epoch 7/32\n",
      "87/87 - 20s - loss: 0.0019 - mae: 0.0293 - 20s/epoch - 230ms/step\n",
      "Epoch 8/32\n",
      "87/87 - 22s - loss: 0.0019 - mae: 0.0293 - 22s/epoch - 257ms/step\n",
      "Epoch 9/32\n",
      "87/87 - 22s - loss: 0.0019 - mae: 0.0293 - 22s/epoch - 250ms/step\n",
      "Epoch 10/32\n",
      "87/87 - 21s - loss: 0.0019 - mae: 0.0293 - 21s/epoch - 238ms/step\n",
      "Epoch 11/32\n",
      "87/87 - 21s - loss: 0.0019 - mae: 0.0293 - 21s/epoch - 244ms/step\n",
      "Epoch 12/32\n",
      "87/87 - 22s - loss: 0.0019 - mae: 0.0293 - 22s/epoch - 257ms/step\n",
      "Epoch 13/32\n",
      "87/87 - 21s - loss: 0.0019 - mae: 0.0293 - 21s/epoch - 238ms/step\n",
      "Epoch 14/32\n",
      "87/87 - 20s - loss: 0.0019 - mae: 0.0293 - 20s/epoch - 233ms/step\n",
      "Epoch 15/32\n",
      "87/87 - 21s - loss: 0.0019 - mae: 0.0293 - 21s/epoch - 240ms/step\n",
      "Epoch 16/32\n",
      "87/87 - 22s - loss: 0.0019 - mae: 0.0293 - 22s/epoch - 247ms/step\n",
      "Epoch 17/32\n",
      "87/87 - 21s - loss: 0.0019 - mae: 0.0293 - 21s/epoch - 236ms/step\n",
      "Epoch 18/32\n",
      "87/87 - 19s - loss: 0.0019 - mae: 0.0293 - 19s/epoch - 220ms/step\n",
      "Epoch 19/32\n",
      "87/87 - 18s - loss: 0.0019 - mae: 0.0293 - 18s/epoch - 206ms/step\n",
      "Epoch 20/32\n",
      "87/87 - 18s - loss: 0.0019 - mae: 0.0293 - 18s/epoch - 206ms/step\n",
      "Epoch 21/32\n",
      "87/87 - 20s - loss: 0.0019 - mae: 0.0293 - 20s/epoch - 231ms/step\n",
      "Epoch 22/32\n",
      "87/87 - 22s - loss: 0.0019 - mae: 0.0293 - 22s/epoch - 250ms/step\n",
      "Epoch 23/32\n",
      "87/87 - 50s - loss: 0.0019 - mae: 0.0293 - 50s/epoch - 576ms/step\n",
      "Epoch 24/32\n",
      "87/87 - 18s - loss: 0.0019 - mae: 0.0293 - 18s/epoch - 211ms/step\n",
      "Epoch 25/32\n",
      "87/87 - 18s - loss: 0.0019 - mae: 0.0293 - 18s/epoch - 201ms/step\n",
      "Epoch 26/32\n",
      "87/87 - 20s - loss: 0.0019 - mae: 0.0293 - 20s/epoch - 235ms/step\n",
      "Epoch 27/32\n",
      "87/87 - 18s - loss: 0.0019 - mae: 0.0293 - 18s/epoch - 210ms/step\n",
      "Epoch 28/32\n",
      "87/87 - 19s - loss: 0.0019 - mae: 0.0293 - 19s/epoch - 217ms/step\n",
      "Epoch 29/32\n",
      "87/87 - 20s - loss: 0.0019 - mae: 0.0293 - 20s/epoch - 225ms/step\n",
      "Epoch 30/32\n",
      "87/87 - 19s - loss: 0.0019 - mae: 0.0293 - 19s/epoch - 216ms/step\n",
      "Epoch 31/32\n",
      "87/87 - 19s - loss: 0.0019 - mae: 0.0293 - 19s/epoch - 221ms/step\n",
      "Epoch 32/32\n",
      "87/87 - 19s - loss: 0.0019 - mae: 0.0293 - 19s/epoch - 218ms/step\n",
      "Our out of folds corr_score is 0.029362713829378554\n",
      "Training fold 10\n",
      "Epoch 1/32\n",
      "96/96 - 26s - loss: 0.0019 - mae: 0.0294 - 26s/epoch - 269ms/step\n",
      "Epoch 2/32\n",
      "96/96 - 19s - loss: 0.0018 - mae: 0.0288 - 19s/epoch - 201ms/step\n",
      "Epoch 3/32\n",
      "96/96 - 20s - loss: 0.0018 - mae: 0.0288 - 20s/epoch - 208ms/step\n",
      "Epoch 4/32\n",
      "96/96 - 20s - loss: 0.0018 - mae: 0.0288 - 20s/epoch - 208ms/step\n",
      "Epoch 5/32\n",
      "96/96 - 19s - loss: 0.0018 - mae: 0.0288 - 19s/epoch - 202ms/step\n",
      "Epoch 6/32\n",
      "96/96 - 20s - loss: 0.0018 - mae: 0.0288 - 20s/epoch - 207ms/step\n",
      "Epoch 7/32\n",
      "96/96 - 18s - loss: 0.0018 - mae: 0.0288 - 18s/epoch - 189ms/step\n",
      "Epoch 8/32\n",
      "96/96 - 17s - loss: 0.0018 - mae: 0.0288 - 17s/epoch - 178ms/step\n",
      "Epoch 9/32\n",
      "96/96 - 18s - loss: 0.0018 - mae: 0.0288 - 18s/epoch - 187ms/step\n",
      "Epoch 10/32\n",
      "96/96 - 17s - loss: 0.0018 - mae: 0.0288 - 17s/epoch - 175ms/step\n",
      "Epoch 11/32\n",
      "96/96 - 16s - loss: 0.0018 - mae: 0.0288 - 16s/epoch - 170ms/step\n",
      "Epoch 12/32\n",
      "96/96 - 17s - loss: 0.0018 - mae: 0.0288 - 17s/epoch - 173ms/step\n",
      "Epoch 13/32\n",
      "96/96 - 16s - loss: 0.0018 - mae: 0.0288 - 16s/epoch - 172ms/step\n",
      "Epoch 14/32\n",
      "96/96 - 16s - loss: 0.0018 - mae: 0.0288 - 16s/epoch - 171ms/step\n",
      "Epoch 15/32\n",
      "96/96 - 17s - loss: 0.0018 - mae: 0.0288 - 17s/epoch - 173ms/step\n",
      "Epoch 16/32\n",
      "96/96 - 16s - loss: 0.0018 - mae: 0.0288 - 16s/epoch - 171ms/step\n",
      "Epoch 17/32\n",
      "96/96 - 17s - loss: 0.0018 - mae: 0.0288 - 17s/epoch - 177ms/step\n",
      "Epoch 18/32\n",
      "96/96 - 20s - loss: 0.0018 - mae: 0.0288 - 20s/epoch - 206ms/step\n",
      "Epoch 19/32\n",
      "96/96 - 21s - loss: 0.0018 - mae: 0.0288 - 21s/epoch - 214ms/step\n",
      "Epoch 20/32\n",
      "96/96 - 20s - loss: 0.0018 - mae: 0.0288 - 20s/epoch - 204ms/step\n",
      "Epoch 21/32\n",
      "96/96 - 20s - loss: 0.0018 - mae: 0.0288 - 20s/epoch - 212ms/step\n",
      "Epoch 22/32\n",
      "96/96 - 21s - loss: 0.0018 - mae: 0.0288 - 21s/epoch - 218ms/step\n",
      "Epoch 23/32\n",
      "96/96 - 20s - loss: 0.0018 - mae: 0.0288 - 20s/epoch - 204ms/step\n",
      "Epoch 24/32\n",
      "96/96 - 20s - loss: 0.0018 - mae: 0.0288 - 20s/epoch - 213ms/step\n",
      "Epoch 25/32\n",
      "96/96 - 19s - loss: 0.0018 - mae: 0.0288 - 19s/epoch - 201ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/32\n",
      "96/96 - 21s - loss: 0.0018 - mae: 0.0288 - 21s/epoch - 219ms/step\n",
      "Epoch 27/32\n",
      "96/96 - 22s - loss: 0.0018 - mae: 0.0288 - 22s/epoch - 230ms/step\n",
      "Epoch 28/32\n",
      "96/96 - 21s - loss: 0.0018 - mae: 0.0288 - 21s/epoch - 216ms/step\n",
      "Epoch 29/32\n",
      "96/96 - 19s - loss: 0.0018 - mae: 0.0288 - 19s/epoch - 195ms/step\n",
      "Epoch 30/32\n",
      "96/96 - 16s - loss: 0.0018 - mae: 0.0288 - 16s/epoch - 169ms/step\n",
      "Epoch 31/32\n",
      "96/96 - 16s - loss: 0.0018 - mae: 0.0288 - 16s/epoch - 167ms/step\n",
      "Epoch 32/32\n",
      "96/96 - 16s - loss: 0.0018 - mae: 0.0288 - 16s/epoch - 169ms/step\n",
      "Our out of folds corr_score is 0.028574102369269862\n",
      "[0.022070941829676012, 0.032396583387459564, 0.07464164040330175, 0.0741484592548737, 0.034470405664758084, -0.06652206518969175, 0.0066864535797939734, 0.009822748663190462, 0.029362713829378554, 0.028574102369269862]\n",
      "Our out of folds corr score is -0.021698813410677656\n"
     ]
    }
   ],
   "source": [
    "test_predictions = train_and_evaluate(new_df,new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          1\n",
       "1          1\n",
       "2          1\n",
       "        ... \n",
       "53889    998\n",
       "53890    998\n",
       "53891    998\n",
       "Name: day, Length: 53892, dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T11:42:08.145003Z",
     "iopub.status.busy": "2023-02-19T11:42:08.144544Z",
     "iopub.status.idle": "2023-02-19T11:42:08.25156Z",
     "shell.execute_reply": "2023-02-19T11:42:08.250527Z",
     "shell.execute_reply.started": "2023-02-19T11:42:08.144966Z"
    },
    "id": "TGGyX9skpJqD"
   },
   "outputs": [],
   "source": [
    "# Save test predictions\n",
    "test['return'] = test_predictions\n",
    "\n",
    "prediction = test[['date_time_x','return']]\n",
    "prediction.columns=['date_time','return']\n",
    "prediction.to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NA8tjThw0EyC"
   },
   "outputs": [],
   "source": [
    "import random \n",
    "random.seed(20230304)\n",
    "\n",
    "SUBMISSION_PATH = '/kaggle/working/submission.csv'\n",
    "\n",
    "POINT_PER_DAY = 50\n",
    "\n",
    "class QIDS:\n",
    "    def __init__(self) -> None:\n",
    "        self.__submission_path = SUBMISSION_PATH\n",
    "        self.__current_idx = 0\n",
    "        self.__predict_idx = 0\n",
    "        self.__num_of_stocks = 54\n",
    "        self.__point_per_day = POINT_PER_DAY\n",
    "        self.__end = False\n",
    "        self.__current_fundamental_df = None\n",
    "\n",
    "        self.__fundamental_df = pd.read_csv(TEST_FUNADMENTAL_PATH)\n",
    "        self.__market_df = pd.read_csv(TEST_MARKET_PATH)\n",
    "        \n",
    "        if len(self.__fundamental_df) / self.__num_of_stocks != len(self.__market_df)/ self.__num_of_stocks / self.__point_per_day:\n",
    "            raise ValueError('The length of fundamental data and market data is not equal.')\n",
    "        self.__length = len(self.__fundamental_df) / self.__num_of_stocks\n",
    "\n",
    "        with open(self.__submission_path, 'w') as f:\n",
    "            f.write('date_time,return\\n') \n",
    "        \n",
    "        print('Environment is initialized.')\n",
    "    \n",
    "    def is_end(self):\n",
    "        return self.__end\n",
    "\n",
    "    # return the fun\n",
    "    def get_current_market(self):\n",
    "        if self.__end:\n",
    "            raise ValueError('The environment has ended.')\n",
    "\n",
    "        # check if the current index is equal to the predict index\n",
    "        if self.__current_idx != self.__predict_idx:\n",
    "            raise ValueError('The current index is not equal to the predict index.')\n",
    "\n",
    "        # load data of the current day\n",
    "        fundamental_df = self.__fundamental_df.iloc[self.__current_idx * self.__num_of_stocks: (self.__current_idx + 1) * self.__num_of_stocks]\n",
    "        market_df = self.__market_df.iloc[self.__current_idx * self.__num_of_stocks * self.__point_per_day: (self.__current_idx + 1) * self.__num_of_stocks * self.__point_per_day]\n",
    "        \n",
    "        # update the current index\n",
    "        self.__current_idx += 1\n",
    "        self.__current_fundamental_df = fundamental_df.reset_index()\n",
    "        \n",
    "        return fundamental_df, market_df\n",
    "\n",
    "    def input_prediction(self, predict_ds: pd.Series):\n",
    "        if self.__end:\n",
    "            raise ValueError('The environment has ended.')\n",
    "\n",
    "        # check if the current index is equal to the predict index plus 1\n",
    "        if self.__current_idx != self.__predict_idx + 1:\n",
    "            raise ValueError('The current index is not equal to the predict index plus 1.')\n",
    "\n",
    "        # check the length of the predict_ds\n",
    "        if len(predict_ds) != self.__num_of_stocks:\n",
    "            raise ValueError('The length of input decisions is wrong.')\n",
    "        \n",
    "        # check the type of the predict_ds\n",
    "        if type(predict_ds) != pd.Series:\n",
    "            raise TypeError('The type of input decisions is wrong.')\n",
    "        \n",
    "        # write the prediction to the submission file\n",
    "        with open(self.__submission_path, 'a') as f:\n",
    "            for idx in range(len(predict_ds)):\n",
    "                f.write(f\"{str(self.__current_fundamental_df['date_time'][idx])},{str(predict_ds.iloc[idx])}\\n\")\n",
    "\n",
    "                # must follow the stock order\n",
    "                # f.write(f\"s{idx}d{self.__current_idx},{str(predict_ds.iloc[idx])}\\n\")\n",
    "        \n",
    "        self.__predict_idx += 1\n",
    "        if self.__predict_idx == self.__length:\n",
    "            self.__end = True\n",
    "            print('Data Feeding is finished.')\n",
    "        \n",
    "\n",
    "# initialize the environment\n",
    "def make_env():\n",
    "    if random.random() == 0.8396457911824297:\n",
    "        return QIDS()\n",
    "    else:\n",
    "        raise ImportError('You cannot make this environment twice.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "id": "-7TOxUH4pJqD",
    "outputId": "ee0fd5dd-62f2-4b4a-f3ee-db9deadf51de"
   },
   "outputs": [],
   "source": [
    "#from qids_package.qids import *\n",
    "\n",
    "env = make_env()\n",
    "\n",
    "import random \n",
    "random.seed(42)\n",
    "\n",
    "while not env.is_end():\n",
    "\tfundamental_df, market_df = env.get_current_market()\n",
    "\t\n",
    "\tl = []\n",
    "\tfor idx in range(54):\n",
    "\t\tl.append(random.random())\n",
    "\tpredict_ds =pd.Series(1)\n",
    "\t\n",
    "\tenv.input_prediction(predict_ds)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
