{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cy1IGAK7pJpz"
   },
   "source": [
    "### This is a simple LGB baseline. You can work for feature engineering.\n",
    "### The seed is 42, which will bring good luck!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-02-19T11:39:13.476656Z",
     "iopub.status.busy": "2023-02-19T11:39:13.476206Z",
     "iopub.status.idle": "2023-02-19T11:39:13.486675Z",
     "shell.execute_reply": "2023-02-19T11:39:13.485328Z",
     "shell.execute_reply.started": "2023-02-19T11:39:13.476619Z"
    },
    "id": "Xs3TRi_HpJqA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import lightgbm as lgb\n",
    "import time\n",
    "import datetime\n",
    "from numba import jit\n",
    "#from lightgbm import LGBMRegressor\n",
    "from multiprocessing import Pool\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "n_fold = 10\n",
    "group_gap = 31\n",
    "seed = 42\n",
    "\n",
    "#Chris' paths:\n",
    "#TRAIN_MARKET_PATH = '/content/drive/MyDrive/hku_qis/hku-qids-2023-quantitative-investment-competition/first_round_train_market_data.csv'\n",
    "#TRAIN_FUNADMENTAL_PATH = '/content/drive/MyDrive/hku_qis/hku-qids-2023-quantitative-investment-competition/first_round_train_fundamental_data.csv'\n",
    "#TRAIN_RETURN_PATH = '/content/drive/MyDrive/hku_qis/hku-qids-2023-quantitative-investment-competition/first_round_train_return_data.csv'\n",
    "\n",
    "#TEST_MARKET_PATH = '/content/drive/MyDrive/hku_qis/hku-qids-2023-quantitative-investment-competition/first_round_test_market_data.csv'\n",
    "#TEST_FUNADMENTAL_PATH = '/content/drive/MyDrive/hku_qis/hku-qids-2023-quantitative-investment-competition/first_round_test_fundamental_data.csv'\n",
    "\n",
    "#Freya's paths:\n",
    "TRAIN_MARKET_PATH = '/Users/75717/Downloads/273_Washu/first_round_train_market_data.csv'\n",
    "TRAIN_FUNADMENTAL_PATH = '/Users/75717/Downloads/273_Washu/first_round_train_fundamental_data.csv'\n",
    "TRAIN_RETURN_PATH = '/Users/75717/Downloads/273_Washu/first_round_train_return_data.csv'\n",
    "\n",
    "TEST_MARKET_PATH = '/Users/75717/Downloads/273_Washu/first_round_test_market_data.csv'\n",
    "TEST_FUNADMENTAL_PATH = '/Users/75717/Downloads/273_Washu/first_round_test_fundamental_data.csv'\n",
    "\n",
    "#Cynthia's paths:\n",
    "#TRAIN_MARKET_PATH = '/content/drive/MyDrive/hku_qis/hku-qids-2023-quantitative-investment-competition/first_round_train_market_data.csv'\n",
    "#TRAIN_FUNADMENTAL_PATH = '/content/drive/MyDrive/hku_qis/hku-qids-2023-quantitative-investment-competition/first_round_train_fundamental_data.csv'\n",
    "#TRAIN_RETURN_PATH = '/content/drive/MyDrive/hku_qis/hku-qids-2023-quantitative-investment-competition/first_round_train_return_data.csv'\n",
    "\n",
    "#TEST_MARKET_PATH = '/content/drive/MyDrive/hku_qis/hku-qids-2023-quantitative-investment-competition/first_round_test_market_data.csv'\n",
    "#TEST_FUNADMENTAL_PATH = '/content/drive/MyDrive/hku_qis/hku-qids-2023-quantitative-investment-competition/first_round_test_fundamental_data.csv'\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 6)\n",
    "pd.set_option('display.max_columns', 350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T11:39:14.994016Z",
     "iopub.status.busy": "2023-02-19T11:39:14.993573Z",
     "iopub.status.idle": "2023-02-19T11:39:20.332244Z",
     "shell.execute_reply": "2023-02-19T11:39:20.331197Z",
     "shell.execute_reply.started": "2023-02-19T11:39:14.993976Z"
    },
    "id": "RdPuh_tdpJqB"
   },
   "outputs": [],
   "source": [
    "#read data\n",
    "df_train_market = pd.read_csv(TRAIN_MARKET_PATH)\n",
    "df_train_return = pd.read_csv(TRAIN_RETURN_PATH)\n",
    "df_train_fundamental = pd.read_csv(TRAIN_FUNADMENTAL_PATH)\n",
    "\n",
    "df_test_market = pd.read_csv(TEST_MARKET_PATH)\n",
    "df_test_fundamental = pd.read_csv(TEST_FUNADMENTAL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T11:39:20.334662Z",
     "iopub.status.busy": "2023-02-19T11:39:20.33424Z",
     "iopub.status.idle": "2023-02-19T11:40:01.127892Z",
     "shell.execute_reply": "2023-02-19T11:40:01.12668Z",
     "shell.execute_reply.started": "2023-02-19T11:39:20.334626Z"
    },
    "id": "Jv4-5wXJpJqB"
   },
   "outputs": [],
   "source": [
    "#merge train dataset and test dataset\n",
    "def split_time(x):\n",
    "    df1 = x['date_time'].str.split('d', expand=True)\n",
    "    df1.columns=['code','s']\n",
    "    code = df1['code']\n",
    "    df1 = df1['s'].str.split('p', expand=True)\n",
    "    df1.columns=['day','time_step']\n",
    "    df2 = x['date_time'].str.rsplit('p', expand=True)\n",
    "    df2.columns=['day_s','s']\n",
    "    df1['day_s'] = df2['day_s']\n",
    "    df1['code'] = code\n",
    "    x = pd.concat([x,df1],axis=1)\n",
    "    \n",
    "    return x\n",
    "\n",
    "df_train_market = split_time(df_train_market)\n",
    "df = pd.merge(df_train_fundamental,df_train_market, left_on='date_time',right_on='day_s')  \n",
    "df = pd.merge(df,df_train_return, left_on='day_s',right_on='date_time')  \n",
    "\n",
    "df_test_market = split_time(df_test_market)\n",
    "test = pd.merge(df_test_fundamental,df_test_market, left_on='date_time',right_on='day_s')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T11:40:01.129791Z",
     "iopub.status.busy": "2023-02-19T11:40:01.129418Z",
     "iopub.status.idle": "2023-02-19T11:40:05.177691Z",
     "shell.execute_reply": "2023-02-19T11:40:05.176402Z",
     "shell.execute_reply.started": "2023-02-19T11:40:01.129759Z"
    },
    "id": "_XWIEwTopJqC"
   },
   "outputs": [],
   "source": [
    "#drop duplicates\n",
    "df = df.drop_duplicates(subset='day_s', keep='last').reset_index(drop=True)\n",
    "test = test.drop_duplicates(subset='day_s', keep='last').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T11:40:05.180387Z",
     "iopub.status.busy": "2023-02-19T11:40:05.179995Z",
     "iopub.status.idle": "2023-02-19T11:40:05.193826Z",
     "shell.execute_reply": "2023-02-19T11:40:05.19235Z",
     "shell.execute_reply.started": "2023-02-19T11:40:05.180348Z"
    },
    "id": "QUpx57DzpJqC"
   },
   "outputs": [],
   "source": [
    "def correlation(a, train_data):\n",
    "    \n",
    "    b = train_data.get_label()\n",
    "    \n",
    "    a = np.ravel(a)\n",
    "    b = np.ravel(b)\n",
    "\n",
    "    len_data = len(a)\n",
    "    mean_a = np.sum(a) / len_data\n",
    "    mean_b = np.sum(b) / len_data\n",
    "    var_a = np.sum(np.square(a - mean_a)) / len_data\n",
    "    var_b = np.sum(np.square(b - mean_b)) / len_data\n",
    "\n",
    "    cov = np.sum((a * b))/len_data - mean_a*mean_b\n",
    "    corr = cov / np.sqrt(var_a * var_b)\n",
    "\n",
    "    return 'corr', corr, True\n",
    "\n",
    "# For CV score calculation\n",
    "def corr_score(pred, valid):\n",
    "    len_data = len(pred)\n",
    "    mean_pred = np.sum(pred) / len_data\n",
    "    mean_valid = np.sum(valid) / len_data\n",
    "    var_pred = np.sum(np.square(pred - mean_pred)) / len_data\n",
    "    var_valid = np.sum(np.square(valid - mean_valid)) / len_data\n",
    "\n",
    "    cov = np.sum((pred * valid))/len_data - mean_pred*mean_valid\n",
    "    corr = cov / np.sqrt(var_pred * var_valid)\n",
    "\n",
    "    return corr\n",
    "\n",
    "# For CV score calculation\n",
    "def wcorr_score(pred, valid, weight):\n",
    "    len_data = len(pred)\n",
    "    sum_w = np.sum(weight)\n",
    "    mean_pred = np.sum(pred * weight) / sum_w\n",
    "    mean_valid = np.sum(valid * weight) / sum_w\n",
    "    var_pred = np.sum(weight * np.square(pred - mean_pred)) / sum_w\n",
    "    var_valid = np.sum(weight * np.square(valid - mean_valid)) / sum_w\n",
    "\n",
    "    cov = np.sum((pred * valid * weight)) / sum_w - mean_pred*mean_valid\n",
    "    corr = cov / np.sqrt(var_pred * var_valid)\n",
    "\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "execution": {
     "iopub.execute_input": "2023-02-19T11:40:19.389021Z",
     "iopub.status.busy": "2023-02-19T11:40:19.387779Z",
     "iopub.status.idle": "2023-02-19T11:40:19.429486Z",
     "shell.execute_reply": "2023-02-19T11:40:19.428403Z",
     "shell.execute_reply.started": "2023-02-19T11:40:19.388979Z"
    },
    "id": "B-9usxYEpJqC",
    "outputId": "37c706cd-3617-438f-9ce0-33cd01b1248b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date_time_x', 'turnoverRatio', 'transactionAmount', 'pe_ttm', 'pe',\n",
       "       'pb', 'ps', 'pcf', 'date_time_y', 'open', 'close', 'high', 'low',\n",
       "       'volume', 'money', 'day', 'time_step', 'day_s', 'code', 'date_time',\n",
       "       'return'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date_time_x', 'turnoverRatio', 'transactionAmount', 'pe_ttm', 'pe',\n",
       "       'pb', 'ps', 'pcf', 'date_time_y', 'open', 'close', 'high', 'low',\n",
       "       'volume', 'money', 'day', 'time_step', 'day_s', 'code'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for normalizing data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "col_train=[i for i in df.columns if i not in ['date_time_x', 'date_time_y', 'day', 'time_step', 'day_s', 'code', 'date_time']]\n",
    "timer_train=df.loc[:,['date_time_x', 'date_time_y', 'day', 'time_step', 'day_s', 'code', 'date_time']]\n",
    "scaled_df=scaler.fit_transform(df[col_train])\n",
    "scaled_df=pd.DataFrame(scaled_df,columns=['turnoverRatio', 'transactionAmount', 'pe_ttm', 'pe', 'pb', 'ps', 'pcf','open', 'close', 'high', 'low', 'volume', 'money','return'])\n",
    "new_df=pd.merge(scaled_df,timer_train,how='outer',left_index=True,right_index=True)\n",
    "\n",
    "col=[i for i in test.columns if i not in ['date_time_x', 'date_time_y', 'day', 'time_step', 'day_s', 'code', 'date_time']]\n",
    "timer=test.loc[:,['date_time_x', 'date_time_y', 'day', 'time_step', 'day_s', 'code']]\n",
    "scaled_test=scaler.fit_transform(test[col])\n",
    "scaled_test=pd.DataFrame(scaled_test,columns=['turnoverRatio', 'transactionAmount', 'pe_ttm', 'pe', 'pb', 'ps', 'pcf','open', 'close', 'high', 'low', 'volume', 'money'])\n",
    "new_test=pd.merge(scaled_test,timer,how='outer',left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T11:42:06.156023Z",
     "iopub.status.busy": "2023-02-19T11:42:06.155626Z",
     "iopub.status.idle": "2023-02-19T11:42:06.169707Z",
     "shell.execute_reply": "2023-02-19T11:42:06.168505Z",
     "shell.execute_reply.started": "2023-02-19T11:42:06.155992Z"
    },
    "id": "jhkIqDnLpJqD"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(train,test):\n",
    "    # Hyperparammeters (just basic)\n",
    "    params = {\n",
    "      'objective': 'rmse',  \n",
    "      'boosting_type': 'gbdt',\n",
    "      'n_jobs': -1,\n",
    "      'verbose': -1\n",
    "    }\n",
    "    \n",
    "    # Split features and target\n",
    "    \n",
    "    x = train[[i for i in df.columns if i not in ['date_time_x', 'date_time_y', 'day', 'time_step', 'day_s', 'code', 'date_time','return']]]\n",
    "    y = train['return']\n",
    "    \n",
    "    x_test = test[[i for i in df.columns if i not in ['date_time_x', 'date_time_y', 'day', 'time_step', 'day_s', 'code', 'date_time','return']]]\n",
    "\n",
    "    oof_predictions = np.zeros(x.shape[0])\n",
    "    test_predictions = np.zeros(x_test.shape[0])\n",
    "    scores = []\n",
    "\n",
    "    # Create a KFold object\n",
    "    gkf = TimeSeriesSplit(n_splits=n_fold,gap=group_gap)\n",
    "    for fold, (trn_ind, val_ind) in enumerate(gkf.split(train['day'].values)):\n",
    "    \n",
    "        print(f'Training fold {fold + 1}')\n",
    "        x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n",
    "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
    "        \n",
    "        #这下面的用到lgb了\n",
    "        train_dataset = lgb.Dataset(x_train, y_train)\n",
    "        val_dataset = lgb.Dataset(x_val, y_val)\n",
    "        model = lgb.train(params = params, \n",
    "                          train_set = train_dataset, \n",
    "                          valid_sets = [train_dataset, val_dataset], \n",
    "                          num_boost_round = 200, \n",
    "                          early_stopping_rounds = 20, \n",
    "                          verbose_eval = False,\n",
    "                          feval = correlation)\n",
    "        # Add predictions to the out of folds array\n",
    "        \n",
    "        oof_predictions[val_ind] = model.predict(x_val)\n",
    "        \n",
    "        rmspe_score = corr_score(y_val,oof_predictions[val_ind])\n",
    "        print(f'Our out of folds corr_score is {rmspe_score}')\n",
    "        scores.append(rmspe_score)\n",
    "        test_predictions += model.predict(x_test) \n",
    "        \n",
    "    rmspe_score = corr_score(y, oof_predictions)\n",
    "    print(scores)\n",
    "    print(f'Our out of folds corr score is {rmspe_score}')\n",
    "    \n",
    "    # Return test predictions\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T11:42:06.156023Z",
     "iopub.status.busy": "2023-02-19T11:42:06.155626Z",
     "iopub.status.idle": "2023-02-19T11:42:06.169707Z",
     "shell.execute_reply": "2023-02-19T11:42:06.168505Z",
     "shell.execute_reply.started": "2023-02-19T11:42:06.155992Z"
    },
    "id": "jhkIqDnLpJqD"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(train,test):\n",
    "    # Hyperparammeters (just basic)\n",
    "    params = {\n",
    "      'objective': 'rmse',  \n",
    "      'boosting_type': 'gbdt',\n",
    "      'n_jobs': -1,\n",
    "      'verbose': -1\n",
    "    }\n",
    "    \n",
    "    # Split features and target\n",
    "    \n",
    "    x = train[[i for i in df.columns if i not in ['date_time_x', 'date_time_y', 'day', 'time_step', 'day_s', 'code', 'date_time','return']]]\n",
    "    y = train['return']\n",
    "    \n",
    "    x_test = test[[i for i in df.columns if i not in ['date_time_x', 'date_time_y', 'day', 'time_step', 'day_s', 'code', 'date_time','return']]]\n",
    "\n",
    "    oof_predictions = np.zeros(x.shape[0])\n",
    "    test_predictions = np.zeros(x_test.shape[0])\n",
    "    scores = []\n",
    "\n",
    "    # Create a KFold object\n",
    "    gkf = TimeSeriesSplit(n_splits=n_fold,gap=group_gap)\n",
    "    for fold, (trn_ind, val_ind) in enumerate(gkf.split(train['day'].values)):\n",
    "    \n",
    "        print(f'Training fold {fold + 1}')\n",
    "        x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n",
    "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
    "        \n",
    "        # create and fit the LSTM network\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1],1)))\n",
    "        model.add(LSTM(units=50))\n",
    "        model.add(Dense(1))\n",
    " \n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        model.fit(x_train, y_train, epochs=1, batch_size=1, verbose=2)\n",
    "        \n",
    "        # Add predictions to the out of folds array\n",
    "        #这个位置报错了\n",
    "        oof_predictions[val_ind] = scaler.inverse_transform(model.predict(x_val))\n",
    "        \n",
    "        rmspe_score = corr_score(y_val,oof_predictions[val_ind])\n",
    "        print(f'Our out of folds corr_score is {rmspe_score}')\n",
    "        scores.append(rmspe_score)\n",
    "        test_predictions += scaler.inverse_transform(model.predict(x_test)) \n",
    "        \n",
    "    rmspe_score = corr_score(y, oof_predictions)\n",
    "    print(scores)\n",
    "    print(f'Our out of folds corr score is {rmspe_score}')\n",
    "    \n",
    "    # Return test predictions\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-02-19T11:42:06.401036Z",
     "iopub.status.busy": "2023-02-19T11:42:06.400583Z",
     "iopub.status.idle": "2023-02-19T11:42:08.132067Z",
     "shell.execute_reply": "2023-02-19T11:42:08.131046Z",
     "shell.execute_reply.started": "2023-02-19T11:42:06.401001Z"
    },
    "id": "s2MYJdoEpJqD",
    "outputId": "bce4f8ed-9413-4f63-8fff-6d27f951369e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "4871/4871 - 30s - loss: 0.0109 - 30s/epoch - 6ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (4899,1) doesn't match the broadcast shape (4899,13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-c64ad65ac5d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnew_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-39-e29bee68e45b>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(train, test)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m# Add predictions to the out of folds array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0moof_predictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_ind\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mrmspe_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorr_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moof_predictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_ind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    459\u001b[0m                         force_all_finite=\"allow-nan\")\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 461\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    462\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: non-broadcastable output operand with shape (4899,1) doesn't match the broadcast shape (4899,13)"
     ]
    }
   ],
   "source": [
    "test_predictions = train_and_evaluate(new_df,new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-19T11:42:08.145003Z",
     "iopub.status.busy": "2023-02-19T11:42:08.144544Z",
     "iopub.status.idle": "2023-02-19T11:42:08.25156Z",
     "shell.execute_reply": "2023-02-19T11:42:08.250527Z",
     "shell.execute_reply.started": "2023-02-19T11:42:08.144966Z"
    },
    "id": "TGGyX9skpJqD"
   },
   "outputs": [],
   "source": [
    "# Save test predictions\n",
    "test['return'] = test_predictions\n",
    "\n",
    "prediction = test[['date_time_x','return']]\n",
    "prediction.columns=['date_time','return']\n",
    "prediction.to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NA8tjThw0EyC"
   },
   "outputs": [],
   "source": [
    "import random \n",
    "random.seed(20230206)\n",
    "\n",
    "SUBMISSION_PATH = '/kaggle/working/submission.csv'\n",
    "\n",
    "POINT_PER_DAY = 50\n",
    "\n",
    "class QIDS:\n",
    "    def __init__(self) -> None:\n",
    "        self.__submission_path = SUBMISSION_PATH\n",
    "        self.__current_idx = 0\n",
    "        self.__predict_idx = 0\n",
    "        self.__num_of_stocks = 54\n",
    "        self.__point_per_day = POINT_PER_DAY\n",
    "        self.__end = False\n",
    "        self.__current_fundamental_df = None\n",
    "\n",
    "        self.__fundamental_df = pd.read_csv(TEST_FUNADMENTAL_PATH)\n",
    "        self.__market_df = pd.read_csv(TEST_MARKET_PATH)\n",
    "        \n",
    "        if len(self.__fundamental_df) / self.__num_of_stocks != len(self.__market_df)/ self.__num_of_stocks / self.__point_per_day:\n",
    "            raise ValueError('The length of fundamental data and market data is not equal.')\n",
    "        self.__length = len(self.__fundamental_df) / self.__num_of_stocks\n",
    "\n",
    "        with open(self.__submission_path, 'w') as f:\n",
    "            f.write('date_time,return\\n') \n",
    "        \n",
    "        print('Environment is initialized.')\n",
    "    \n",
    "    def is_end(self):\n",
    "        return self.__end\n",
    "\n",
    "    # return the fun\n",
    "    def get_current_market(self):\n",
    "        if self.__end:\n",
    "            raise ValueError('The environment has ended.')\n",
    "\n",
    "        # check if the current index is equal to the predict index\n",
    "        if self.__current_idx != self.__predict_idx:\n",
    "            raise ValueError('The current index is not equal to the predict index.')\n",
    "\n",
    "        # load data of the current day\n",
    "        fundamental_df = self.__fundamental_df.iloc[self.__current_idx * self.__num_of_stocks: (self.__current_idx + 1) * self.__num_of_stocks]\n",
    "        market_df = self.__market_df.iloc[self.__current_idx * self.__num_of_stocks * self.__point_per_day: (self.__current_idx + 1) * self.__num_of_stocks * self.__point_per_day]\n",
    "        \n",
    "        # update the current index\n",
    "        self.__current_idx += 1\n",
    "        self.__current_fundamental_df = fundamental_df.reset_index()\n",
    "        \n",
    "        return fundamental_df, market_df\n",
    "\n",
    "    def input_prediction(self, predict_ds: pd.Series):\n",
    "        if self.__end:\n",
    "            raise ValueError('The environment has ended.')\n",
    "\n",
    "        # check if the current index is equal to the predict index plus 1\n",
    "        if self.__current_idx != self.__predict_idx + 1:\n",
    "            raise ValueError('The current index is not equal to the predict index plus 1.')\n",
    "\n",
    "        # check the length of the predict_ds\n",
    "        if len(predict_ds) != self.__num_of_stocks:\n",
    "            raise ValueError('The length of input decisions is wrong.')\n",
    "        \n",
    "        # check the type of the predict_ds\n",
    "        if type(predict_ds) != pd.Series:\n",
    "            raise TypeError('The type of input decisions is wrong.')\n",
    "        \n",
    "        # write the prediction to the submission file\n",
    "        with open(self.__submission_path, 'a') as f:\n",
    "            for idx in range(len(predict_ds)):\n",
    "                f.write(f\"{str(self.__current_fundamental_df['date_time'][idx])},{str(predict_ds.iloc[idx])}\\n\")\n",
    "\n",
    "                # must follow the stock order\n",
    "                # f.write(f\"s{idx}d{self.__current_idx},{str(predict_ds.iloc[idx])}\\n\")\n",
    "        \n",
    "        self.__predict_idx += 1\n",
    "        if self.__predict_idx == self.__length:\n",
    "            self.__end = True\n",
    "            print('Data Feeding is finished.')\n",
    "        \n",
    "\n",
    "# initialize the environment\n",
    "def make_env():\n",
    "    if random.random() == 0.8396457911824297:\n",
    "        return QIDS()\n",
    "    else:\n",
    "        raise ImportError('You cannot make this environment twice.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "id": "-7TOxUH4pJqD",
    "outputId": "ee0fd5dd-62f2-4b4a-f3ee-db9deadf51de"
   },
   "outputs": [],
   "source": [
    "#from qids_package.qids import *\n",
    "\n",
    "env = make_env()\n",
    "\n",
    "import random \n",
    "random.seed(42)\n",
    "\n",
    "while not env.is_end():\n",
    "\tfundamental_df, market_df = env.get_current_market()\n",
    "\t\n",
    "\tl = []\n",
    "\tfor idx in range(54):\n",
    "\t\tl.append(random.random())\n",
    "\tpredict_ds =pd.Series(1)\n",
    "\t\n",
    "\tenv.input_prediction(predict_ds)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
